{"./":{"url":"./","title":"简介","keywords":"","body":"前言 用来记录学习python的点滴。 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"django/django.html":{"url":"django/django.html","title":"django","keywords":"","body":"Django [TOC] django相关知识。 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"nlp/NLP.html":{"url":"nlp/NLP.html","title":"nlp","keywords":"","body":"NLP [TOC] 零、常用知识点 一、语料预处理 1. 数据洗清 2. 分词 3. 去停用词 4. 词性标注 二、特征工程 1. 特征提取 2. 特征选择 三、词法与语法分析 1. 浅层分析 2. 语法分析 四、模型训练 五、文本摘要 1. 关键短语提取 2. 主题建模 3. 自动文档摘要 六、文本相似度 1. 词项相似度分析 2. 文档相似度分析 七、文本分类/聚类 1. 文本分类 2. 文本聚类 八、情感分析 九、可视化 零、常用知识点 单词 -> 短语 -> 从句 -> 句子 -> 文档/语料库 单词/标识（token）：是具有一定的句法语义，且独立的最小文本成分。 词：一门独立语言中的最小单位。 词素：具有独特意义的最小语言单位，但它不是独立的，一个单词可以由几个词素组成。 词元：是一组词的基本形式。 词位{eating，ate，eats} 包含3种词性，他们的词元是eat。 词性标注（POS）：Part-Of-Speech tagging 命名实体识别（NER）：Named Entity Recognition 过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。 欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。 有汉字成词能力的 HMM 模型 NLP工具小清单 GATE、Mallet、Open NLP、UIMA、Stanford toolkit、Gensim、NLTK NLP常用python库 NLTK pattern.en：包含文本分析的大多实用工具。 gensim：具有一套丰富的语义分析功能，包括主题建模和相似性分析。word2vec模型的py接口。 textblob：文本处理，短语提取，分类，pos标注，文本翻译，情感分析 spacy：提供工业级NPL功能 其他：不是专门用于文本分析的框架和库，但当相对文本使用机器学习时，是有用的 scikit-learn numpy scipy stack 深度学习和基于张量的库：theano，tensorflow，keras 一、语料预处理 在一个完整的中文NLP应用中，语料预处理占到整个工作量的50%-70%。 1. 数据洗清 把不感兴趣的、视为噪音的内容清洗删除。主要有： 链接 以@开头的用户名 人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。 2. 分词 在进行分词前，需要进行命名实体提取，这样分词更准确。 常见的分词算法： 基于字符串匹配的分词方法：在已有字典的基础上，按照指定的规则进行匹配，直到完成规则中的“最大”匹配，则识别出一个词。按照匹配的方向不同可以分为：正向最大匹配、逆向最大匹配、双向最大匹配。 基于理解的分词方法：利用计算机模拟人对文本的理解，结合语义、句法等因素处理文本，从而实现分词。该方法需要大量的语言知识，由于中文文本自身的复杂性，该方法目前还难以实施。 基于统计的分词方法：计算机通过计算字符串在语料库的出现频率对其是否构成词进行判断。使用最为广泛。 当前中文分词算法的主要难点有歧义识别和新词识别。 (0) 常用的中文分词工具 常见的中文分词实现：中科院计算所 NLPIR、哈工大 LTP、清华大学 THULAC 、斯坦福分词器、Hanlp 分词器、jieba 分词、IKAnalyzer 等。 常用的分词词典：《同义词词林（扩展版）》、《现代汉语语义词典》、《现代汉语语义词典》、《知网》、《人民日报语料库》等。 ICTCLAS 分词系统，其主要思想是通过隐马尔可夫模型进行分词，实现已有词识别、简单未登录词识别、词性标注等功能，提高了分词的准确性和效率。 NLPIR 分词工具也是以此为基础开发的，缺点是标准版本需要付费，提供的接口难以适用于JAVA。 THULAC 分词工具是由清华大学自然语言处理与社会人文计算实验室研发，具备分词和词性标注等功能，计算能力强、速度快、准确率高，缺点是只支持UTF8 编码的中文文本。 分词改进方向： 提高未登录词的识别率 歧义消除 提高分词速度 在目前实际应用中，广泛使用的分词手段还是已有的分词工具进行初步分词，再结合未登录词识别算法进一步进行操作。 《00 中文文本分类方法综述_于游》 ​ 针对未登录词识别问题，文献[8]提出网络舆情中的新词识别方法，利用网络舆情中未被词典收录的主题词的局部高频这一特性，通过计算异常分词与周围分词之间的粘结度，识别出未被词典收录的主题词，但该方法仅仅通过单个字分词对异常分词进行判断和召回。文献[9]针对短文本的特点，通过对条件随机场中的标记选择和特征做出了优化，提出一种基于条件随机场的中文文本分词方法，该算法可有效解决传统CRF 算法标记冗余的问题，并有良好的未登录词识别效果，但由于标记选择的原因，其在不同长度词的识别上有一定的局限。文献[10]对未登录词识别方法做了进一步改进，利用互信息改进算法，提出一种非监督的词识别方法，结合规则，可以在大规模语料中识别出指定长度的新词。文献[11]和文献[12]都通过LSTM 记忆单元和神经网络模型，对分词方法进行了改进，改进后的方法有效利用序列长距离信息和上下文信息，但算法复杂且神经网络具有黑箱特性，不易于理解。 (1) jieba分词 Jieba 分词工具是基于Trie 树结构采用动态规划查找最大概率路径的方法得到分词结果，并采用基于隐马尔可夫模型和Viterbi 算法进行未登录词识别，是国内使用最多的中文分词工具。缺点是在未登录词识别上存在缺陷，大部分需要用户手动加入词典。 jieba分词步骤： 基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）。 基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词。 对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。 3. 去停用词 停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气助词、人称代词等。 在一般性的文本处理中，分词之后，下一步就是去停用词。但是对于中文来说，是否去停用词、去除哪些停用词，是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的。 4. 词性标注 给每个词或者词语打词类标签。 词性标注不是必需的。比如，文本分类就不用关心词性问题，而类似情感分析、知识推理却是需要的。 词性标注方法 基于规则的词性标注方法 基于统计的词性标注方法 基于最大熵的词性标注、基于统计最大概率输出词性、基于 HMM 的词性标注 jieba # pos segment # return: [jieba.posseg.pair(word, flag), ] jieba.posseg.cut(sentence) 二、特征工程 特征提取主要是通过属性间的关系，改变原特征空间，如组合不同属性得到新的属性。有PCA 、LDA 、SVD。 特征选择则是对原特征空间中的特征进行筛选，没有改变其原属性。有Filter 、Wrapper 、Embedded 。 两者的核心目的都是为降低特征向量维度。 1. 特征提取 特征提取的一般方法： Filter 法：主要思想是通过对每个特征赋予权重，根据其重要程度对特征进行选择。目前常用的Filter 法主要有：基于文档频率的方法、x2统计量法、互信息方法、信息增益方法。 Wrapper 法：实质是将特征选择问题作为寻优的问题，通过对不同组合进行评价和比较，选择出最优的特征集合。目前常用的Wrapper 方法主要有遗传算法（GA ）、粒子群优化（PSO ）、优化蚁群算法（ACO ）。 Embedded 法：通过在建立模型的过程中，筛选出对提高模型准确度最有用的特征。 (1) 词袋模型（Bag of Word, BOW) 不考虑词语顺序，直接对词语出现的次数进行统计。 统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。 (2) TF-IDF模型（Token Frequency-Inverse Document Frequency） 由词频*逆文档频率得到。 词频（Token Frequency）：表示词 t 在文档 d 中出现的频率。 逆文档频率（Inverse Document Frequency）：表示语料库中包含词 t 的文档的数目的倒数。 (3) 词向量模型 词向量就是要用某个固定维度的向量去表示单词。 将字、词语转换成向量矩阵的计算模型。 常用模型： One-hot：把每个词表示为一个很长的向量。向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。 Word2Vec：将 One-Hot Encoder 转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词也将被映射到向量空间中相近的位置。经过降维，在二维空间中，相似的单词在空间中的距离也很接近。 Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。 Word2Vec包含2中模型： 跳字模型（Skip-Gram） 连续词袋模型（Continuous Bag of Words，CBOW） Doc2Vec：Doc2Vec 是 Mikolov 在 Word2Vec 基础上提出的另一个用于计算长文本向量的工具。 实现方式：DBOW（Distributed Bag of Words）、 DM （Distributed Memory）。 WordRank TextRank FastText GloVe 在工业界，Word2Vec 和 Doc2Vec 常见的应用有：做相似词计算；相关词挖掘，在推荐系统中用在品牌、用户、商品挖掘中；上下文预测句子；机器翻译；作为特征输入其他模型等。 2. 特征选择 文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。因此，选用特征向量，我们要找这样的向量：即使它丢失了一些语义信息，这些丢失的语义信息也不是我们需要的，或占比权重很小的。 在一个实际问题中，在构造好的特征向量中，是要选择合适的、表达能力强的特征，即与你要解决的问题最相关的特征向量。 常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。 三、词法与语法分析 1. 浅层分析 浅层句法分析：用来提取句子（一组单词）中的短语。 2. 语法分析 (1) 依存语法（基于词的语法） 在大多数情况下，将动词视为句子的根。依存关系树不是描述句子中词的顺序，而是强调句子中词之间的关系。 from pyhanlp import HanLP # 依存句法分析 HanLP.parseDependency(document) (2) 成分语法（短语结构文法） 四、模型训练 朴素贝叶斯 ? 朴素贝叶斯应用在自然语言处理的时候，是不是只是把单词截断当作特征，全然没有考虑语序的问题啊。有没有那样的句子，比如把主谓宾调换位置，语义就变了呢？是不是朴素贝叶斯在自然语言处理方面有这种缺陷呢 支持向量机（SVM） 模型训练 有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型 深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。 模型训练需注意：注意过拟合、欠拟合问题，不断提高模型的泛化能力。 过拟合解决方法： 增大数据的训练量。 增加正则化项，如 L1 正则和 L2 正则。 特征选取不合理，人工筛选特征和使用特征选择算法。 采用 Dropout 方法等。 欠拟合解决方法： 添加其他特征项。 增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强。 减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。 对于神经网络，注意梯度消失和梯度爆炸问题。 模型的评价指标 混淆矩阵： 真实情况 预测 结果 正例 负例 正例 TP（真正例） FN（假负例） 负例 FP（假正例） TN（真负例） 注：正例，是我们感兴趣的类。Positive，Negative 具体指标： 错误率：分类错误的样本数占样本总数的比例。 精度：分类正确的样本数占样本总数的比例。 错误率 + 精度 = 1 准确率（accuracy）：(TP+TN)/ALL。预测正确的占比。 精确率（precision）：TP/ALL。真正例占比。 召回率（recall）：TP/(TP+FN)。正例中有多少正例被准确预测。 F1 衡量：精确率与召回率的调和平均值。 ROC曲线：受试者工作特征（Receiver Operating Characteristic）曲线。纵轴是“真正例率”（真正例/实际正例），横轴是“假正例率”（假正例/实际负例）。 AUC（Area Under Curve）：ROC 曲线下的面积。 ROC 曲线的意义有以下几点： ROC 曲线能很容易的查出任意阈值对模型的泛化性能影响； 有助于选择最佳的阈值； 可以对不同的模型比较性能，在同一坐标中，靠近左上角的 ROC 曲所代表的学习器准确性最高。 五、文本摘要 1. 关键短语提取 关键短语提取通常是执行更复杂任务的起点。最终提取出来的是一个词汇集。 (1) 搭配 搭配是一种趋向频繁发生的序列或一组词。 可以使用原始频率、点互信息（pointwise mutual information， PMI）等度量方法来找到搭配。 (2) 基于权重标签的短语提取 步骤： 使用浅层分析提取所有的名词短语词块。 计算每个词块的TF-IDF权重，返回最大加权短语。 关键字提取 jieba.analyse.extract_tags：使用TF-IDF提取关键字 jieba.analyse.textrank：使用textRank算法提取关键字 基于 LDA 主题模型进行关键词提取 步骤为：文件加载 -> jieba 分词 -> 去停用词 -> 构建词袋模型 -> LDA 模型训练 -> 结果可视化。 from pyhanlp import HanLP # 提取关键字 HanLP.extractKeyword(document, 2) 2. 主题建模 3. 自动文档摘要 from pyhanlp import HanLP # 自动摘要 HanLP.extractSummary(document, 3) 六、文本相似度 1. 词项相似度分析 汉明距离 曼哈顿距离 欧几里得距离 莱文斯坦编辑距离 余弦距离 2. 文档相似度分析 余弦相似度 海灵格-巴塔恰亚距离 Okapi BM25排名 七、文本分类/聚类 1. 文本分类 文本分类是指利用计算机按照一定的分类标准或体系自动将文本分门别类，它不仅是自然语言处理问题，也是一个模式识别问题。 文本分类的核心内容： 文本表示：向量空间模型VSM（如词袋模型、tf-idf模型）、统计模型（基本思路是挖掘文本的主题信息）。主题模型是当前文本表示研究的主要范式，LDA 模型是其典型代表 分类模型：KNN、SVM、AdaBoost 文本分类一般包括文本预处理、分词、模型构建和分类几个过程。 文本分类一般分为5步： 文本 -> 预处理 -> 分词（未登录词识别） -> 特征提取/特征选择 -> 文本表示 -> 文本分类 -> 结果 文本预处理：去掉文本中多余的部分，如标点、介词等。 分词：对预处理后的文本进行词切分操作，并识别其中的未登录词。 特征提取和特征选择：得到文本分词结果后，选择文本特征提取方法，并对特征进行选择，约简特征，尽量降低维度，减少后续计算量。 文本表示：选择合适的方法表示选择的特征，作为分类的依据。 文本分类：选择合理的分类方法对文本进行分类，得到文本类别。 在文本分类中，分词方法、特征选择以及分类算法是关键，主要也是对这几方面进行改进。 文本分类的一般方法： 基于知识工程（KE，knowledge engineering）的分类方法 基于机器学习（ML，machine learning）的分类方法。朴素贝叶斯、SVM、神经网络、决策树。 2. 文本聚类 应用无监督的ML概念和技术，将文档分成不同的类别。 类之间总会有一些重叠，并没有一个完美聚类的定义。 文本聚类是将一个个文档由原有的自然语言文字信息转化成数学信息，以高维空间点的形式展现出来，通过计算哪些点距离比较近，从而将那些点聚成一个簇，簇的中心叫做簇心。 一个好的聚类要保证簇内点的距离尽量的近，但簇与簇之间的点要尽量的远。 k-means聚类 近邻传播聚类 沃德凝聚层次聚类 八、情感分析 九、可视化 在文本领域用的比较多的可视化类型有： 基于文本内容的可视化：包括基于词频的可视化，基于词汇分布的可视化。常用的有词云、分布图和 Document Cards 等。 基于文本关系的可视化：研究文本内外关系，帮助人们理解文本内容和发现规律。常用的可视化形式有树状图、节点连接的网络图、力导向图、叠式图和 Word Tree 等。 基于多层面信息的可视化：研究如何结合信息的多个方面帮助用户从更深层次理解文本数据，发现其内在规律。其中，包含时间信息和地理坐标的文本可视化近年来受到越来越多的关注。常用的有地理热力图、ThemeRiver、SparkClouds、TextFlow 和基于矩阵视图的情感分析可视化等。 前端可视化学习网站： 百度的 Echarts，基于 Canvas，适合刚入门的新手，遵循了数据可视化的一些经典范式，只要把数据组织好，就可以轻松得到很漂亮的图表。 D3.js，基于 SVG 方便自己定制，D3 V4 支持 Canvas+SVG，D3.js 比 Echarts 稍微难点，适合有一定开发经验的人。 three.js，是一个基于 WebGL 的 3D 图形的框架，可以让用户通过 JavaScript 搭建 WebGL 项目。 import networkx as nx import matplotlib.pyplot as plt # 设置matplotlib正常显示中文,用黑体显示中文 plt.rcParams['font.sans-serif']=['SimHei'] plt.rcParams['axes.unicode_minus']=False colors = ['red', 'green', 'blue', 'yellow'] #有向图 DG = nx.DiGraph() DG.add_node('A') DG.add_node('B') #添加边，有方向，A-->B DG.add_edge('A','B') #一次性添加多节点，输入的格式为列表 DG.add_nodes_from(['甘肃', '武威', '兰州', '张掖','凉州区']) #添加边，数据格式为列表 DG.add_edges_from([('甘肃', '武威'), ('甘肃', '兰州'), ('甘肃','张掖'),('武威','凉州区')]) #作图，设置节点名显示,节点大小，节点颜色 nx.draw(DG,with_labels=True, node_size=900, node_color = colors) plt.show() Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"nlp/NLP 相关算法.html":{"url":"nlp/NLP 相关算法.html","title":"NLP 相关算法","keywords":"","body":"NLP 相关算法 [TOC] 文本预处理(分词) HMM 模型 CRF模型 特征向量 词袋模型（BOW） tf-idf算法 分词 结巴分词 hanlp分词 文本摘要 模型 文本聚类 降维工具 文本分类 情感分析 文本预处理(分词) HMM（隐马尔可夫模型）和 CRF（条件随机场）算法常常被用于分词、句法分析、命名实体识别、词性标注等。 两者之间有很大的共同点，所以在很多应用上往往是重叠的，但在命名实体、句法分析等领域 CRF 似乎更胜一筹。 生成式模型：P(Y|X)= P(X,Y)/ P(X)。估计的是联合概率分布P(Y, X)。主要关心的是给定输入X，产生输出 Y 的生成关系。 判别式模型：P(Y, X)=P(Y|X)*P(X)。估计的是条件概率分布 P(Y|X)。主要关心的是对于给定的输入 X，应该预测什么样的输出 Y。 生成式模型和判别式模型都用于有监督学习 模型： 生成式模型：HMM、Gaussian、 Naive Bayes、Mixtures of multinomials 等。 判别式模型：CRF、K 近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法等。 HMM 模型 CRF模型 特征向量 词袋模型（BOW） 把文本（段落或者文档）看作是无序的词汇集合，忽略语法甚至是单词的顺序，把每一个单词都进行统计，同时计算每个单词出现的次数，常常被用在文本分类中，如贝叶斯算法、LDA 和 LSA 等。 from gensim import corpora #tokenized是[[token,],[token,]] dictionary = corpora.Dictionary(tokenized) #保存词典 dictionary.save('deerwester.dict') dictionary.token2id # 得到单词与id的映射 dictionary.doc2bow(sentence) # 将[token, ]转化为稀疏矩阵[(index,count), ] from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer( analyzer='word', # tokenise by character ngrams max_features=4000, # keep the most common 1000 ngrams ) # 根据corpus创造相应的词典 vec.fit(corpus) # corpus = [str,str]，str会被tokenize vec.vocabulary_ # 得到词语与id的映射 vec.transform(corpus) # 将curpos转化为向量，稀疏矩阵 vec.transform(corpus).todense() # numpy的稠密矩阵 tf-idf算法 from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer #将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频 vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5) #统计每个词语的tf-idf权值 transformer = TfidfTransformer() # ?第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵 tfidf = transformer.fit_transform(vectorizer.fit_transform(centences)) # centences是[str,] vectorizer.get_feature_names() # 获取词袋模型中的所有词语组成的list tfidf.toarray() textrank算法 词向量Word2Vec from gensim.models import Word2Vec model = Word2Vec( sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None, ) \"\"\" sentences: [[token,], [token,]] sg: sg=1 是skip-gram算法，对低频词敏感。默认sg=0为CBOW算法。 size: 是输出词向量的维数，一般值取为100到200之间。 window: 是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）。 min_count: 对词进行过滤，频率小于min-count的单词则会被忽视，默认值为5。 negative: 和sample可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。 hs: hs=1表示层级softmax将会被使用。默认hs=0且 negative 不为0，则负采样将会被选择使用。 \"\"\" # 训练后的模型可以保存与加载 model.save('model') #保存模型 model = Word2Vec.load('model') #加载模型 Doc2Vec 在 Gensim 库中，Doc2Vec 与 Word2Vec 都极为相似。但两者在对输入数据的预处理上稍有不同，Doc2vec 接收一个由 LabeledSentence 对象组成的迭代器作为其构造函数的输入参数。LabeledSentence 是 Gensim 内建的一个类，它接收两个 List 作为其初始化的参数：word list 和 label list。 Doc2Vec 也包括两种实现方式：DBOW（Distributed Bag of Words）和 DM （Distributed Memory）。 dm = 0 或者 dm=1 决定调用 DBOW 还是 DM。 from gensim.models.doc2vec import Doc2Vec,LabeledSentence model = Doc2Vec(dm=1, size=100, window=8, min_count=5, workers=4) model.build_vocab(iter_data) # iter_data = yield LabeledSentence([token,] ,label) model.train(iter_data,total_examples=model.corpus_count,epochs=1000,start_alpha=0.01,end_alpha =0.001) # start_alpha 为开始学习率, 大于end_alpha #根据标签找最相似的 model.docvecs.most_similar(xx) 分词 from gensim import corpora #构建词袋模型 dictionary = corpora.Dictionary(sentences) corpus = [dictionary.doc2bow(sentence) for sentence in sentences] 结巴分词 全模式分词：把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义。 import jieba jieba.cut(sentence, cut_all=False, HMM=True) \"\"\" return: generator. - sentence: The str(unicode) to be segmented. - cut_all: Model type. True for full pattern(所有可能的分词结果都扫描出来), False for accurate pattern(将句子最精确地切开,唯一切分). - HMM: Whether to use the Hidden Markov Model. \"\"\" # return: list. jieba.lcut(sentence, cut_all=False, HMM=True) jieba添加新词： jieba.add_word(new_word) # 添加原词典中没有的 jieba.load_userdict('user_dict.txt') # 使用文件 hanlp分词 命令行使用： > hanlp segment # 进入交互分词模式 > hanlp serve # 启动内置HTTP服务器，http://localhost:8765 分词： from pyhanlp import HanLP # return： jpype._jclass.java.util.ArrayList HanLP.segment(sentence) 添加自定义新词： from pyhanlp import CustomDictionary CustomDictionary.add(\"黄钢\") CustomDictionary.insert(\"新乡信息港\", \"nz 1024\") CustomDictionary.add(\"交易平台\", \"nz 1024 n 1\") 文本摘要 # 基于 tf-idf 提取关键字 jieba.analyse.extract_tags( sentence, topK=20, withWeight=False, allowPOS=(), withFlag=False, ) \"\"\" sentence：待提取的文本语料. topK：返回 TF-IDF 权重最大的关键词个数. withWeight：是否需要返回关键词的权重值. allowPOS：仅包括指定词性的词，默认值为空，即不筛选。 withFlag：当allowPOS不为空时，withFlag=True返回[(jieba.posseg.pair(word, flag), weight), ] \"\"\" # 基于 TextRank 算法提取关键词 jieba.analyse.textrank( sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'), withFlag=False, ) # 基于 LDA 主题模型提取关键词 # 构建词袋模型，sentences是[[token,],[token,]] dictionary = corpora.Dictionary(sentences) corpus = [dictionary.doc2bow(sentence) for sentence in sentences] # LDA模型，num_topics是主题的个数 lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10) lda.print_topic(1, topn=5) from pyhanlp import HanLP # 内部采用 TextRankKeyword 实现 result = HanLP.extractKeyword(sentence, 20) # java.util.ArrayList print(result) 模型 HMM模型 LDA主题模型 gensim.models.ldamodel.LdaModel #构建词袋模型 dictionary = corpora.Dictionary(sentences) corpus = [dictionary.doc2bow(sentence) for sentence in sentences] #lda模型，num_topics是主题的个数 lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10) 贝叶斯分类模型： # 分别进行算法建模和模型训练。 from sklearn.naive_bayes import MultinomialNB classifier = MultinomialNB() classifier.fit(vec.transform(vec_train), label_train) classifier.score(vec.transform(vec_test), label_test) SVM分类模型： from sklearn.svm import SVC svm = SVC(kernel='linear') svm.fit(vec.transform(vec_train), label_train) svm.score(vec.transform(vec_test), label_test) 决策树、随机森林、XGBoost、神经网络等 文本聚类 k-means模型： from sklearn.cluster import KMeans from sklearn.decomposition import PCA # n_clusters需要手动指定，分为4类，这里也可以选择随机初始化init=\"random\" clf = KMeans(n_clusters=4, max_iter=10000, init=\"k-means++\", tol=1e-6) pca = PCA(n_components=10) # 降维 TnewData = pca.fit_transform(weight) # 将weight变为10维的向量 s = clf.fit(TnewData) # 训练模型 clf.cluster_centers_ # 聚类后的[中心向量,] 采用基于密度的 DBSCAN、层次聚类等算法 降维工具 PCA： from sklearn.decomposition import PCA pca = PCA(n_components=10) # 降维 TnewData = pca.fit_transform(weight) # 将weight变为10维的向量 TSNE: from sklearn.manifold import TSNE ts =TSNE(10) newData = ts.fit_transform(weight) 两者同为降维工具，主要区别在于，所在的包不同（也即机制和原理不同）。 因为原理不同，导致 TSNE 保留下的属性信息，更具代表性，也即最能体现样本间的差异，但是 TSNE 运行极慢，PCA 则相对较快。 文本分类 序列数据的处理，我们从语言模型 N-gram 模型说起，然后着重谈谈 RNN，并通过 RNN 的变种 LSTM 和 GRU 来实战文本分类。 RNN：循环神经网络（Recurrent Neural Network） LSTM GRU（Gated Recurrent Unit），简化版的 LSTM。 情感分析 中文情感分析方法简介； SnowNLP 快速进行评论数据情感分析； 基于标注好的情感词典来计算情感值； pytreebank 绘制情感树； 情感倾向可认为是主体对某一客体主观存在的内心喜恶，内在评价的一种倾向。它由两个方面来衡量：一个情感倾向方向，一个是情感倾向度。 情感倾向分析： 基于情感词典的方法：需要用到标注好的情感词典。 基于机器学习的方法，如基于大规模语料库的机器学习。需要大量的人工标注的语料作为训练集，通过提取文本特征，构建分类器来实现情感的分类。 文本情感分析的分析粒度可以是词语、句子、段落或篇章。 该把句子中词语的依存关系纳入到句子情感的计算过程中去，不同的依存关系，进行情感倾向计算是不一样的。 SnowNLP库 主要可以进行中文分词、词性标注、情感分析、文本分类、转换拼音、繁体转简体、提取文本关键词、提取摘要、分割句子、文本相似等。 用 SnowNLP 进行情感分析，官网指出进行电商评论的准确率较高，其实是因为它的语料库主要是电商评论数据，但是可以自己构建相关领域语料库，替换单一的电商评论语料，准确率也挺不错的。 行业标准的情感词典——玻森情感词典 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"python/python.html":{"url":"python/python.html","title":"python","keywords":"","body":"python [TOC] 记录python基础知识。 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"python/python其他库.html":{"url":"python/python其他库.html","title":"python其他库","keywords":"","body":"python其他库 [TOC] argparse jinja2 codecs pipreqs colorama：命令行输出彩色文字 curses csv相关 excel相关 word相关 mongodb mysql redis OCR GUI库：PyQt5 进度条：tqdm kazoo：zookeeper API argparse import argparse if __name__=='__main__': parser = argparse.ArgumentParser(description=\"Demo of argparse\") # 参数解析器 parser.add_argument('cmd', type=str, nargs='?', help='help info') parser.add_argument('-n', '--name', default=' Li ') parser.add_argument('-y', '--year', default='20') parser.add_argument('--debug', action=\"store_true\") # 不需要跟值，使用该选项就会使debug属性为True。如果不加该参数，debug值默认False。'store_false'则相反 args = parser.parse_args() print(args) name = args.name year = args.year print('Hello {} {}'.format(name, year)) nargs 参数来限定输入的位置参数的个数，默认为1。当然nargs参数也可用于普通带标签的参数。 nargs 字符串： '*'：之后所有的输入都将作为该位置参数的值 ‘+’：表示读取至少1个该位置参数。 '?'：表示该位置参数要么没有，要么就只要一个。 jinja2 Jinja2是Python下一个被广泛应用的模版引擎，他的设计思想来源于Django的模板引擎，并扩展了其语法和一系列强大的功能。其中最显著的一个是增加了沙箱执行功能和可选的自动转义功能，这对大多应用的安全性来说是非常重要的。 Flask使用jinja2作为框架的模板系统。使用django等其他Python web框架也可以方便的集成jinja2模板系统 如果将减号（-）添加到块的开头或结尾（例如For标签），注释或变量表达式，则将删除该块之前或之后的空格。 from jinja2 import Template template = Template('Hello {{ name }}!') template.render(name='John Doe') # Out: 'Hello John Doe!' jinja2可以操作列表（元组 ）、字典、对象。 列表： li.0 li[0] {% for i in range(10) %} {{ i }} ｛% endfor %} codecs codecs : Python Codec Registry, API and helpers. coder, decoder python的内部是使用unicode来处理的，但是unicode的使用需要考虑的是它的编码格式有两种，一是UCS-2，它一共有65536个码 位，另一种是UCS-4，它有2147483648g个码位。 可以通过以下代码查看： import sys print(sys.maxunicode) import codecs fw = codecs.open('test1.txt','a','utf-8') fw.write(line2) open(filename, mode='r', encoding=None, errors='strict', buffering=1) pipreqs pipreqs可以找到当前项目使用的所有python包及其版本。 # 1.在项目根目录下执行命令 pipreqs ./ # 报错就执行下面这条 pipreqs ./ --encoding=utf-8 # 2.可以看到在根目录下生成了requirements.txt # 3.执行下面代码就会把项目用到的所有组件装上 pip3 install -r requirements.txt colorama：命令行输出彩色文字 colorama是一个python专门用来在控制台、命令行输出彩色文字的模块，可以跨平台使用。 colorama内部模块：Fore是针对字体颜色，Back是针对字体背景颜色，Style是针对字体格式。 Fore: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET. Back: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET. Style: DIM, NORMAL, BRIGHT, RESET_ALL init()接受一些 kwargs覆盖缺省行为， autoreset是自动恢复到默认颜色。 from colorama import init, Fore, Back init(autoreset=True) print(Fore.BLUE + \"hgg\") print(Fore.YELLOW + Back.RED + \"XXXX\") print(\"hello world\") curses python 中curses封装了c语言的curses。curses是一个在linux/unix下广泛应用的图形函数库，作用是可以在终端内绘制简单的图形用户界面。curses库不支持Windows操作系统，但有一个非正式curses包可以尝试，另外windows平台可以使用Console模块。 curses的名字起源于\"cursor optimization\"，即光标优化。 可以说，curses是Linux终端图形编程的不二选择（比如著名的文字编辑器 vi 就是基于curses编的）。 csv相关 python标准库自带CSV模块。 import csv csvfile = open('csv_test.csv', 'r') # 以列表形式输出 reader = csv.reader(csvfile) rows = [row for row in reader] # 以字典形式输出，第一行作为字典的键 reader = csv.DictReader(csvfile) for row in reader: # row 是一个 dict # 写文件 # 若存在文件，打开csv文件，若不存在即新建文件 # 如不设置newline=''，每行数据会隔一行空白行 csvfile = open('csv_test.csv', 'w', newline='') # 将文件加载到csv对象中 writer = csv.writer(csvfile) # 写入一行数据 writer.writerow(['姓名', '年龄', '电话']) # 多行数据写入 data = [ ('小P', '18', '138001380000'), ('小Y', '22', '138001380000') ] writer.writerows(data) # 关闭csv对象 csvfile.close() excel相关 python操作excel的有xlrd（xls read），xlwt（xls write）。 pip install xlrd pip install xlwt import xlwt # 新建一个Excel文件 wb = xlwt.Workbook() # 新建一个Sheet ws = wb.add_sheet('Python', cell_overwrite_ok=True) # 定义格式对象 style = xlwt.XFStyle() # 合并单元格write_merge(开始行, 结束行, 开始列, 结束列, 内容, 格式) ws.write_merge(0, 0, 0, 5, 'Python', style) # 写入数据wb.write(行,列,内容) for i in range(2, 7): for k in range(5): ws.write(i, k, i+k) # Excel公式xlwt.Formula ws.write(i, 5, xlwt.Formula('SUM(A'+str(i+1)+':E'+str(i+1)+')')) # 插入图片，insert_bitmap(img, x, y, x1, y1, scale_x=0.8, scale_y=1) # 图片格式必须为bmp # x表示行数，y表示列数 # x1表示相对原来位置向下偏移的像素 # y1表示相对原来位置向右偏移的像素 # scale_x，scale_y缩放比例 ws.insert_bitmap('test.bmp', 9, 1, 2, 2, scale_x=0.3, scale_y=0.3) # 保存文件 wb.save('file.xls') import xlrd wb = xlrd.open_workbook('file.xls') # 获取Sheets总数 ws_count = wb.nsheets # 通过索引顺序获取Sheets # ws = wb.sheets()[0] # ws = wb.sheet_by_index(0) # 通过Sheets名获取Sheets ws = wb.sheet_by_name('Python') # 获取整行的值（以列表返回内容） row_value = ws.row_values(3) # 获取整列的值（以列表返回内容） row_col = ws.col_values(3) # 获得行列数 nrows = ws.nrows ncols = ws.ncols # 获取某个单元格内容cell(行, 列) cell_F3 = ws.cell(2, 5).value # 使用行列索引获取某个单元格内容 row_F3 = ws.row(2)[5].value col_F3 = ws.col(5)[2].value word相关 pip install python-docx import docx from docx.shared import Inches doc = docx.Document('test.docx') # 读取全部内容 paras = doc.paragraphs # 处理每一个段落（以\\n划分段落） for p in paras: # xxx # 创建对象 document = Document() # 添加正文内容并设置部分内容格式 p = document.add_paragraph('Python 爬虫开发-') # 设置内容加粗 p.runs[0].bold = True p.add_run('存储实例。').italic = True # 添加正文，设置'样式'-'明显引用' document.add_paragraph('样式-明显引用', style='IntenseQuote') # 添加图片 document.add_picture('test.png', width=Inches(1.25)) # 添加表格 table = document.add_table(rows=1, cols=3) hdr_cells = table.rows[0].cells hdr_cells[0].text = 'Qty' # 保存文件 document.add_page_break() document.save('test.docx') mongodb 可视化工具：RoboMongo，MongoBooster。 pip install pymongo import pymongo # 创建对象 client = pymongo.MongoClient() # client = pymongo.MongoClient('localhost', 27017) # client = pymongo.MongoClient('mongodb://localhost:27017/') # 进行认证 db_auth = client.admin db_auth.authenticate(username, password) # 用户验证方法2 # client = pymongo.MongoClient('mongodb://username:password@localhost:27017/') # 连接DB数据库 db = client['DB'] # 连接集合user，集合类似关系数据库的数据表。如果集合不存在，会新建集合user user_collection = db.user # 插入数据，user_info是一个dict user_id = user_collection.insert_one(user_info).inserted_id # 批量添加，user_infos是一个关于dict的列表 user_collection.insert_many(user_infos) 更新文档 $set：指定键值，不存在则创建。 $unset：从文档中移除指定的键。 $inc：进行算术加减操作，只用于整数、长整数、双精度浮点数。 $rename：重命名字段名称 $push：如果指定的键存在，就向已有的数组末尾添加一个元素。否则创建一个新的数组。 $and、$or：后面跟数组。 $lt、$lte、$gt、$gte、$in、$nin：比较操作符。 $regex：正则表达式 mysql pip install SQLAlchemy pip install pymysql from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker # 创建数据表方法一 from sqlalchemy import Column, Integer, String, DateTime from sqlalchemy.ext.declarative import declarative_base Base = declarative_base() class mytable(Base): # 表名 __tablename__ = 'mytable' # 字段，属性 id = Column(Integer, primary_key=True) name = Column(String(50), unique=True) age = Column(Integer) birth = Column(DateTime) class_name = Column(String(50)) Base.metadata.create_all(engine) # 创建数据表方法二 from sqlalchemy import Column, MetaData, ForeignKey, Table from sqlalchemy.dialects.mysql import (INTEGER, CHAR) meta = MetaData() myclass = Table('myclass', meta, Column('id', INTEGER, primary_key=True), Column('name', CHAR(50), ForeignKey(mytable.name)), Column('class_name', CHAR(50)) ) myclass.create(bind=engine) engine = create_engine(\"mysql+pymysql://root:1990@localhost:3306/test?charset=utf8\",echo=True) DBSession = sessionmaker(bind=engine) session = DBSession() new_data = mytable(name='Li Lei',age=10,birth='2017-10-01',class_name='一年级一班') session.add(new_data) session.commit() session.query(mytable).filter_by(id=1).update({ mytable.age : 12}) session.commit() get_data = session.query(myclass.name, myclass.class_name).all() get_data = session.query(myclass).filter_by(id=1).all() # 内连接 get_data = session.query(mytable).join(myclass).filter(mytable.class_name == '三年级二班').all() # 外连接 get_data = session.query(mytable).outerjoin( myclass).filter(mytable.class_name == '三年级二班').all() sql = 'select * from mytable ' session.execute(sql) session.close() redis OCR pip install pyocr pip install Pillow Pillow处理图像。 from PIL import Image from pyocr import tesseract im = Image.open('1.png') im = im.convert('L') # 图片转换为灰色图像, 转化后识别率更高 # 保存转换后的图片 im.save(\"temp.png\") code = tesseract.image_to_string(im) print(code) GUI库：PyQt5 pip install PyQt5 pip install PyQt5-tools 安装PyQt5-tools后，在Lib/site-packages/pyqt5-tools中找到designer.exe，用其进行界面设计。 将ui文件转换为py文件。 python -m PyQt5.uic.pyuic xxx.ui -o xxx_v.py 进度条：tqdm tqdm：阿拉伯语中的“process“，对循环或者迭代器，显示进度条。 from tqdm import tqdm import time for i in tqdm(range(19), desc=\"进度条\"): time.sleep(0.5) kazoo：zookeeper API 建立连接： from kazoo.client import KazooClient host = 'localhost:2181' zk = KazooClient(host) zk.start() # 相关操作 zk.stop() 获得某个节点的信息，返回一个元组： zk.get('/') # Out[10]:(b'', ZnodeStat(czxid=0, mzxid=4, ctime=0, mtime=1558407882652, version=1, cversion=4, aversion=1, ephemeralOwner=0, dataLength=0, numChildren=6, pzxid=288)) 获取某个节点的所有字节点，返回一个子节点名称的列表： zk.get_children('/') # Out[11]: ['default', 'route', 'zookeeper', 'test', 'config'] 获取某个节点的acl（access control list），返回一个元组： zk.get_acls('/') # Out[12]: ([ACL(perms=31, acl_list=['ALL'], id=Id(scheme='world', id='anyone')), ACL(perms=31, acl_list=['ALL'], id=Id(scheme='digest', id='fsi:NLfFQJbUxUqJLJOtUIyVEQrlNeM='))], ZnodeStat(czxid=0, mzxid=4, ctime=0, mtime=1558407882652, version=1, cversion=4, aversion=1, ephemeralOwner=0, dataLength=0, numChildren=6, pzxid=288)) 创建一个节点，返回一个路径字符串： zk.create('/test/hg_test/test2/test3/node',b'hello world') Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 21:05:27 "},"python/python常用库.html":{"url":"python/python常用库.html","title":"python常用库","keywords":"","body":"python常用库 [TOC] collections namedtupe functools partial 时间相关 time datetime logging yaml collections namedtupe 跟tuple相比，namedtuple 返回的子类可以使用名称来访问元素。 from collections import namedtuple Point = namedtuple('点',['x', 'y']) a = Point(1,5) # 这里只能传2个参数 print(a) # Out: 点(x=1, y=5) print(a.x) # Out: 1 # 为namedtupe 赋予默认值 Point = namedtuple('点',['x', 'y']) Field.__new__.__defaults__ = (0, 0) Field.__new__.__defaults__ = (0,) # 如果赋值的数量少于原有数量，则会从右往左进行赋值。 在这里是将y属性赋值为0 functools partial functools.partial(func[,*args][, **kwargs]) 用来对函数func的某些参数进行默认设置。 def multiply(x, y): return x * y double = functools.partial(multiply, y=2) double(5) # Out: 10 时间相关 Python 提供了一个 time 和 calendar 模块可以用于格式化日期和时间。还有datetime模块 time time.time() 返回一个时间戳（浮点数），代表从1970-01-01 00:00:00 到现在经过了多少秒。 import time time.time() # Out[5]: 1585811480.8246994 time.localtime()，可以接受一个时间戳（默认为当前时间），返回一个时间元组，如下图所示： time.localtime() # Out[6]: time.struct_time(tm_year=2020, tm_mon=4, tm_mday=2, tm_hour=15, tm_min=13, tm_sec=28, tm_wday=3, tm_yday=93, tm_isdst=0) （年，月，日，时，分，秒，一周的第几天（0是周一），一年中的第几天，是否是夏令时） dst：夏令时（Daylight Saving time） 格式化输出时间： time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) # Out[7]: '2020-04-02 15:15:20' datetime 引入datetime： # datetime是一个模块，在里面有一个类叫datetime from datetime import datetime 获取当前时间： datetime.now() # Out[9]: datetime.datetime(2020, 4, 2, 15, 17, 33, 188997) 格式化时间： datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") # Out[10]: '2020-04-02 15:19:27' 获取本月第一天和最后一天： import calendar import datetime from datetime import timedelta this_month_start = datetime.datetime(now.year, now.month, 1) this_month_end = datetime.datetime(now.year, now.month, calendar.monthrange(now.year, now.month)[1]) logging 级别排序:CRITICAL > ERROR > WARNING > INFO > DEBUG debug : 打印全部的日志,详细的信息,通常只出现在诊断问题上 info : 打印info,warning,error,critical级别的日志,确认一切按预期运行 warning : 打印warning,error,critical级别的日志,一个迹象表明,一些意想不到的事情发生了,或表明一些问题在不久的将来(例如。磁盘空间低”),这个软件还能按预期工作 error : 打印error,critical级别的日志,更严重的问题,软件没能执行一些功能 critical : 打印critical级别,一个严重的错误,这表明程序本身可能无法继续运行 典型的日志记录的步骤是这样的： 创建logger 创建handler（用于写入日志文件，或输出到控制台） 定义formatter（定义handler的输出格式（formatter）） 给handler添加formatter 给logger添加handler fmt： %(name)s Name of the logger (logging channel) %(levelno)s Numeric logging level for the message (DEBUG, INFO, WARNING, ERROR, CRITICAL) %(levelname)s Text logging level for the message (\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\") %(pathname)s Full pathname of the source file where the logging call was issued (if available) %(filename)s Filename portion of pathname %(module)s Module (name portion of filename) %(lineno)d Source line number where the logging call was issued (if available) %(funcName)s Function name %(created)f Time when the LogRecord was created (time.time() return value) %(asctime)s Textual time when the LogRecord was created %(msecs)d Millisecond portion of the creation time %(relativeCreated)d Time in milliseconds when the LogRecord was created, relative to the time the logging module was loaded (typically at application startup time) %(thread)d Thread ID (if available) %(threadName)s Thread name (if available) %(process)d Process ID (if available) %(message)s The result of record.getMessage(), computed just as the record is emitted import logging # 1、创建一个logger logger = logging.getLogger('mylogger') logger.setLevel(logging.DEBUG) # 2、创建一个handler，用于写入日志文件 fh = logging.FileHandler('test.log') fh.setLevel(logging.DEBUG) # 再创建一个handler，用于输出到控制台 ch = logging.StreamHandler() ch.setLevel(logging.DEBUG) # 3、定义handler的输出格式（formatter） formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') # 4、给handler添加formatter fh.setFormatter(formatter) ch.setFormatter(formatter) # 5、给logger添加handler logger.addHandler(fh) logger.addHandler(ch) yaml python使用yaml： import yaml f = open('cfg.yaml', 'rt', encoding='utf8') # ? rt模式下，python在读取文本时会自动把\\r\\n转换成\\n. cfg = yaml.load(f_cfg) s = \"\"\" s: hello li: - 1 - 黄钢 \"\"\" yaml.load(s) # {'s': 'hello', 'li': [1, '黄钢']} 主要有3种类型： 对象：键值对的集合 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 纯量（scalars）：单个的、不可再分的值 字符串 布尔值：isSet: true 或 isSet: True 整数 浮点数 Null：parent: ~ 或 parent: null 时间 日期 注意： 大小写敏感 使用缩进表示层级关系 缩进时不允许使用Tab键，只允许使用空格。 缩进的空格数目不重要，只要相同层级的元素左侧对齐即可 强制转换类型：isTrue: !!str True 引用 与解耦： defaults: &defaults adapter: postgres host: localhost development: database: myapp_development Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 20:57:22 "},"python/python环境.html":{"url":"python/python环境.html","title":"python环境","keywords":"","body":"python环境 [TOC] python编辑器 python交互 python ipython jupyter ipython与jupyter的关系 jupyterlab python 虚拟环境 virtualenv virtualenvwrapper conda python编辑器 在windows上，安装python后会自带IDLE编辑器，能用，但一般般。 集成开发一般用PyCharm，功能十分强大。 python交互 python 最简单的方式是直接执行python命令，会启动一个python命令行，可以执行相关代码。但是该方式操作有些不方便。 ipython 功能介绍： Tab补全 func?：快速查询文档，相当于help(func)。 !shell_cmd：在shell命令前加上感叹号，直接执行shell命令，无需退出ipython。还可以赋值 a = !pwd # Out[8]: ['/root'] type(a) # Out[10]: IPython.utils.text.SList 魔术方法（以%开头）： %run python_file_path：运行py文件。 %time func()：查看func函数的运行时间。 %lsmagic：列出所有魔术方法。 jupyter jupyter是一种交互式计算和开发环境的笔记，支持多语言，输出图形、音频、视频等功能。 安装： pip3 install jupyter 启动： jupyter notebook # 启动一个web服务，用于交互。是基于ipython的 配置多个内核 一般情况下，我安装了jupyter之后，只有一个python环境。比如，我是使用python3启动的jupyter notebook，我就只能使用python3。但我想使用python2怎么办呢？使用如下方法： 首先，要有python2的环境，假设python2启动的是python2相关的环境。 python2 -m pip install ipykernel # 安装 IPython Kernel for Jupyter python2 -m ipykernel install # 将该python2的ipython内核添加进jupyter配置中 此时就能使用python2和python3两个环境了。 查看jupyter的ipython内核： jupyter kernelspec list # 输出 Available kernels: python3 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\resources python2 C:\\ProgramData\\jupyter\\kernels\\python2 # 列出jupyter的设置、数据目录 jupyter --paths 有时为避免冲突，需要指定唯一的名称 python -m ipykernel install --name myenv --display-name 'py27\" --name 是给jupyter 启动Kernel 使用（是jupyter kernelspec list显示在前面的name，如果指定的name已存在则会覆盖。--display-name 是为Jupyter notebook 菜单显示。 jupyter扩展 安装nbextensions： pip install jupyter_contrib_nbextensions jupyter contrib nbextension install --user # 安装之后还需要启动扩展 jupyter nbextensions_configurator enable --user --user表示将扩展安装到当前用户的配置文件夹下。 然后重启jupyter，在弹出的主页面里，能看到增加了一个Nbextensions标签页，在这个页面里，可以勾选相关扩展从而启动扩展。 常用扩展： Hinterland：每敲完一个键，就出现下拉菜单，可以直接选中你需要的词汇。 Snippets：在工具栏里加了一个下拉菜单，可以非常方便的插入代码段（预先编写好的，类似模板）。 Autopep8：使python代码符合pep8规范，需要先安装依赖pip install autopep8。 搭建 jupyter notebook 服务 首先在linux上安装好python及相应的包，然后进行下面的操作。 编辑配置文件：~/.jupyter/jupyter_notebook_config.py：（当前用户关于jupyter的配置文件） 如果没有，就先生成： jupyter notebook --generate-config 修改配置： c.NotebookApp.allow_remote_access = True c.NotebookApp.ip = '*' c.NotebookApp.password = 'sha1:xxx:xxx' c.NotebookApp.open_browser = False c.NotebookApp.port =8888 #可自行指定一个端口, 访问时使用该端口 自己设置一个密码： # 启动ipython from notebook.auth import passwd passwd() 将生成的'sha1:xxx:xxx'写入到配置文件中的 c.NotebookApp.password 变量中。 后台启动jupyter botebook，并将日志写入指定文件： nohup jupyter notebook --allow-root &>jupyter.log & 设置nginx代理： server { listen 80; server_name python.opstar.club; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Is-EDU 0; location / { proxy_pass http://127.0.0.1:8888; } error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 重启nginx服务： systemctl restart nginx.service nginx日志存放在： /var/log/nginx/access.log ipython与jupyter的关系 ipython最初是一个python的交互式解释器，随着ipython的不断发展，它的组件与具体的编程语言逐渐解耦。IPython 3.x 是IPython的最后一个独立发行版，包含了notebook服务器、qtconsole等。 从IPython 4.0 开始，项目中与语言无关的部分：the notebook format、 message protocol、 qtconsole、notebook web application 等都转移到了新的项目中，并命名为Jupyter。 而IPython本身专注于交互式Python，其中一部分是为Jupyter提供Python内核。 jupyterlab jupyterlab是jupyter notebook的加强版。 pip install jupyterlab # 安装 jupyter-lab # 运行 python 虚拟环境 创建虚拟环境： python3.6 -m venv myvenv 创建关于python3.6，名为myvenv虚拟环境 virtualenv virtualenv是用来创建虚拟环境的软件工具，virtualenvwrapper软件包可以让我们更加方便的管理虚拟环境。 pip install virtualenv 创建虚拟环境： virtualenv -p C:\\Python36\\python.exe venv36 使用C:\\Python36\\python.exe这个python来创建一个名为venv36的python虚拟环境，如果使用参数-p的话，会使用默认的python创建虚拟环境。 激活虚拟环境： cd venv36 # 进入虚拟环境目录 source bin/activate # linux Scripts\\activate.bat # windows # 退出虚拟环境 deactivate virtualenvwrapper 安装： pip install virtualenvwrapper==4.8.4 # 未知原因，不指定版本下载失败 pip install virtualenvwrapper-win # 提供关于virtualenvwrapper的windows接口 配置系统变量： 在windows下，配置系统变量WORKON_HOME，以后创建的虚拟环境文件就放在该目录下。如果没有配置该变量，则在哪个目录下创建虚拟环境，虚拟环境的文件就会放在那个目录下。 相关命令： # mkvirtualenv [--python==C:\\Python36\\python.exe] 虚拟环境名 # --python参数可省略 mkvirtualenv venv36 # 使用默认python创建一个名为venv36的虚拟环境 workon # 列出已有的虚拟环境 lsvirtualenv # 列出已有的虚拟环境 workon venv36 # 激活venv36虚拟环境 deactivate # 退出当前虚拟环境 rmvirtualenv venv36 # 移除venv36虚拟环境 cdvirtualenv venv36 # 进入虚拟环境目录。 注：mkvirtualenv命令创建虚拟环境时，会共享原python的一些文件，即创建的虚拟环境中有些文件是快捷方式（或符号链接）。从而cdvirtualenv命令进入的目录可能不是按照虚拟环境文件的目录。 如果不想使用快捷方式，可以执行如下命令： mkvirtualenv --copies env_copy conda 创建虚拟环境： conda create -n learn_python36 python=3.6 conda env list Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 21:12:27 "},"python/基础知识.html":{"url":"python/基础知识.html","title":"基础知识","keywords":"","body":"基础知识 [TOC] 参数 循环的技巧 字符串的format方法 python 变量 变量详解 常用运算符 控制语句 内置函数 python模块与包 python异常 类 参数 主要有2种参数： 位置参数 关键字参数：类似键值对。 在函数调用中，关键字参数必须跟随在位置参数的后面。 调用函数时可使用的正式参数类型： 必备参数：必备参数须以正确的顺序传入函数。 关键字参数：类似于键值对，允许函数调用时参数的顺序与声明时不一致。 默认参数：默认参数的值如果没有传入，则被认为是默认值 不定长参数：前面加个* 当存在一个形式为 **kwargs 的正式形参时，它会接收一个字典，其中包含除了与正式形参相对应的关键字参数以外的所有关键字参数。 一个形式为 *args，接收一个包含除了正式形参列表以外的位置参数的元组。 循环的技巧 遍历字典，用 items() 方法可将关键字和对应的值同时取出。 遍历列表，用 enumerate() 函数可以将索引位置和其对应的值同时取出。 如果要按某个指定顺序循环一个序列，可以用 sorted()函数，它可以在不改动原序列的基础上返回一个新的排列。 当同时在两个或更多序列中循环时，可以用 zip() 函数将其内元素一一匹配。 questions = ['name', 'quest', 'favorite color'] answers = ['lancelot', 'the holy grail', 'blue'] for q, a in zip(questions, answers): print('What is your {0}? It is {1}.'.format(q, a)) 当逆向循环一个序列时，先正向定位序列，然后调用 reversed()函数 for i in reversed(range(5)): print(i) 字符串的format方法 格式如下： { : } 格式控制标记： ： , 填充的单个字符 >：右对齐 ^：居中对齐 设置输出宽度 数字的千分位分隔符 浮点数的精度 整数：b/c/d/o/x/X浮点数：e/E/f/% python 变量 变量命名规则： 以单下划线开头 _foo的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用from xxx import *导入。 以双下划线开头的 foo 代表类的私有成员。 以双下划线开头和结尾的 _foo 代表 Python 里特殊方法专用的标识，如 init_() 代表类的构造函数。 Python有五个标准的数据类型： Numbers（数字）：是不可改变的数据类型，这意味着改变数字数据类型会分配一个新的对象。数据类型是不允许改变的,这就意味着如果改变 Number 数据类型的值，将重新分配内存空间。 String（字符串） List（列表）： 是 Python 中使用最频繁的数据类型。 Tuple（元组）：元组不能二次赋值，相当于只读列表。 Dictionary（字典）：最灵活的内置数据结构类型。列表是有序的对象集合，字典是无序的对象集合。 Python支持四种不同的数字类型： int（有符号整型） long（长整型[也可以代表八进制和十六进制]）: 32L,231l float（浮点型） complex（复数）： a+bj, complex(a,b) 。复数的实部 a 和虚部 b 都是浮点型。 注意：long 类型只存在于 Python2.X 版本中，在 2.2 以后的版本中，int 类型数据溢出后会自动转为long类型。在 Python3.X 版本中 long 类型被移除，使用 int 替代。 Python 可以同一行显示多条语句，方法是用分号 ; 分开。 Python语句中一般以新行作为语句的结束符。但是我们可以使用斜杠（ \\）将一行的语句分为多行显示，如下所示： total = item_one + \\ item_two + \\ item_three 语句中包含 [], {} 或 () 括号就不需要使用多行连接符。如下实例： days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] Python 可以使用引号( ' )、双引号( \" )、三引号( ''' 或 \"\"\" ) 来表示字符串。 其中三引号可以由多行组成，编写多行文本的快捷语法，常用于文档字符串。在文件的特定地点，可以被当做注释。 Python允许你同时为多个变量赋值。例如： a = b = c = 1 以上实例，创建一个整型对象，值为1，三个变量被分配到相同的内存空间上。 您也可以为多个对象指定多个变量。例如： a, b, c = 1, 2, \"hello\" 以上实例，两个整型对象 1 和 2 分别分配给变量 a 和 b，字符串对象 \"hello\" 分配给变量 c。 可以使用del语句删除一些对象的引用。 del var1[,var2[,var3[....,varN]]]] 加号（+）是字符串连接运算符，星号（*）是重复操作。 str * 2 # 输出字符串两次 str + \"TEST\" # 输出连接的字符串 列表的操作和字符串类似。 变量详解 每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。 每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 等号（=）运算符左边是一个变量名，右边是存储在变量中的值。 变量是存储在内存中的值。这就意味着在创建变量时会在内存中开辟一个空间。 基于变量的数据类型，解释器会分配指定内存，并决定什么数据可以被存储在内存中。因此，变量可以指定不同的数据类型，这些变量可以存储整数，小数或字符。 字典：键必须不可变，所以可以用数字，字符串或元组充当，所以用列表就不行。 在 python 中，strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象。 对象类型： 不可变类型：变量赋值 a=5 后再赋值 a=10，这里实际是新生成一个 int 值对象 10，再让 a 指向它，而 5 被丢弃，不是改变a的值，相当于新生成了a。 可变类型：变量赋值 la=[1,2,3,4] 后再赋值 la[2]=5 则是将 list la 的第三个元素值更改，本身la没有动，只是其内部的一部分值被修改了。 python 函数的参数传递： 不可变类型：类似 c++ 的值传递，如 整数、字符串、元组。如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身。 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响 python 中一切都是对象，严格意义我们不能说值传递还是引用传递，我们应该说传不可变对象和传可变对象。 如果一个局部变量和一个全局变量重名，则局部变量会覆盖全局变量。如果要给函数内的全局变量赋值，必须使用 global 语句。 常用运算符 算术运算符：加（+）、减（-）、乘（）、除（/）、取余（%）、幂（*）、整除（//）。 位运算符：&，|，^（异或），~（取反），> 逻辑运算符：and，or，not 成员运算符：in，not in 注： id() 函数用于获取对象内存地址。 is 与 == 区别： is 用于判断两个变量引用对象是否为同一个(同一块内存空间)， == 用于判断引用变量的值是否相等。 控制语句 在 python 中，while … else 在循环条件为 false 时执行 else 语句块： for … else 表示这样的意思，for 中的语句和普通的没有区别，else 中的语句会在循环正常执行完（即 for 不是通过 break 跳出而中断的）的情况下执行，while … else 也是一样。 内置函数 dir()返回的列表容纳了在一个模块里定义的所有模块，变量和函数。 显示所有内置函数/变量/类： import builtins print(dir(builtins)) 根据调用地方的不同，globals() 和 locals() 函数可被用来返回全局和局部命名空间里的名字。 如果在函数内部调用 locals()，返回的是所有能在该函数里访问的命名。 如果在函数内部调用 globals()，返回的是所有在该函数里能访问的全局名字。 python模块与包 Python 模块(Module)，是一个 Python 文件，以 .py 结尾。模块能定义函数，类和变量，模块里也能包含可执行的代码。 包是一个分层次的文件目录结构，它定义了一个由模块及子包，和子包下的子包等组成的 Python 的应用环境。 简单来说，包就是文件夹，但该文件夹下必须存在 __init__.py 文件(内容可以为空)。__init__.py 用于标识当前文件夹是一个包。 当一个模块被导入到一个脚本，模块顶层部分的代码只会被执行一次。因此，如果你想重新执行模块里顶层部分的代码，可以用 reload() 函数 import importlib importlib.reload(module_name) 当解释器遇到 import 语句，如果模块在当前的搜索路径中，就会被导入。 搜索路径是一个解释器会先进行搜索的所有目录的列表。 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 具有该名称的内置模块 当前目录 如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 当导入一个包时，搜索顺序 和导入模块差不多，但是没有步骤1（最先搜索内置模块）。 如以下代码所示： html/ parser.py html/ __init__.py parser.py main.py # main.py from html import parser # other code 第1个是html是模块（没有定义 __init__.py），第2个是包。当使用模块时，main.py中导入parser会先到内置模块中寻找，而正好有个同名的内置模块html，最终导致导入的是内置模块html，而不是自己编写的。 如果使用第二种写法，则导入的是自己写的html.parser。 python异常 BaseException 所有异常的基类 Exception 常规错误的基类 raise语法格式如下： # raise [Exception [, args [, traceback]]] # 定义函数 def mye( level ): if level 类 class ClassName: '类的帮助信息' #类文档字符串 class_suite #类体 类的帮助信息可以通过ClassName.doc查看。 class_suite 由类成员，方法，数据属性组成。 Python内置类属性： dict : 类的属性（包含一个字典，由类的数据属性组成） doc :类的文档字符串 name: 类名 module: 类定义所在的模块（类的全名是'main.className'，如果类位于一个导入模块mymod中，那么className.module 等于 mymod） bases : 类的所有父类构成元素（包含了一个由所有父类组成的元组） Python 使用了引用计数这一简单技术来跟踪和回收垃圾。在 Python 内部记录着所有使用中的对象各有多少引用。 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 20:20:18 "},"python/小技巧.html":{"url":"python/小技巧.html","title":"小技巧","keywords":"","body":"小技巧 [toc] format简略写法 __slots__ 通过URL打开图片 字符串解码 format简略写法 print('a+b={}'.format(a+b)) print(f'a+b={a+b}') __slots__ python是动态语言，可以在运行的过程中，修改代码。如果想限制实例的属性，可以使用 __slots__。 class Person: __slots__ = (\"name\", \"age\") Person类只有name/age2个属性可以赋值。如果想增加其他属性，会出错。 注：__slots__ 定义的属性仅对当前类实例起作用，对继承的子类不起作用 通过URL打开图片 import requests as req from PIL import Image from io import BytesIO img_src = 'http://wx2.sinaimg.cn/mw690/ac38503ely1fesz8m0ov6j20qo140dix.jpg' response = req.get(img_src) image = Image.open(BytesIO(response.content)) image.show() 字符串解码 因为decode的函数原型是 decode([encoding], [errors='strict']) ，可以用第二个参数控制错误处理的策略，默认的参数就是strict，代表遇到非法字符时抛出异常。 其他选项： ignore：忽略非法字符 replace：会用?取代非法字符 xmlcharrefreplace：使用XML的字符引用 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 21:02:41 "},"python/常用知识点.html":{"url":"python/常用知识点.html","title":"常用知识点","keywords":"","body":"python 常用知识点 [TOC] 字符编码 进制转换 重新加载某些模块 遍历文件（os模块） python -m xxx 与 python xxx.py 的区别 字符编码 全角与半角：简单来说，对于英文字母、数字、符号而言，全角占用2个标准字符的位置，而半角只占用1一个标准字符的位置． 对于英文字母、数字、符号而言（二者相差65248）： 半角字符unicode编码从33~126 。 全角字符unicode编码从65281~65374 。 空格比较特殊，全角为 12288，半角为 32。 可以自行编码进行二者之间的转换，如下面代码所示： chr获取数字对应得unicode字符，ord获取unicode字符对应的编码。 In [54]: chr(123) Out[54]: '{' In [55]: ord('{') Out[55]: 123 In [56]: ord('我') Out[56]: 25105 In [57]: chr(25105) Out[57]: '我' 也可以使用： import unicodedata unicodedata.normalize('NFKC', '：') # 将后面字符串中的全角字符转换为半角字符 进制转换 python中有内置函数进行转换。 将十进制数字转化为其他进制的字符串： In [41]: str(32) # 10进制 Out[41]: '32' In [25]: hex(32) # 16进制 Out[25]: '0x20' In [26]: oct(32) # 8进制 Out[26]: '0o40' In [27]: bin(32) # 2进制 out[27]: '0b100000' 将其他进制的字符串转化为10进制数字： In [33]: int('20',16) # 将字符串按16进制进行解释 Out[33]: 32 In [34]: int('20',10) Out[34]: 20 In [35]: int('20',8) Out[35]: 16 重新加载某些模块 import some_module #防止reload报错 import importlib importlib.reload(some_module) 在ipython中可以使用以下命令，可以自动重新加载修改过的文件： %load_ext autoreload %autoreload 2 注：要在启动ipython时执行 遍历文件（os模块） def getFiles(path): list = os.listdir(path) #列出文件夹下所有的目录与文件 for i in range(0,len(list)): path_ = os.path.join(path,list[i]) if os.path.isfile(path_): #你想对文件的操作 或者使用os.walk方法： def walkFiles(path): for root, dirs, files in os.walk(path): # root 表示当前正在访问的文件夹路径 # dirs 表示该文件夹下的子目录名list # files 表示该文件夹下的文件list # 遍历文件 for f in files: print(os.path.join(root, f)) # 遍历所有的文件夹 for d in dirs: print(os.path.join(root, d)) python -m xxx 与 python xxx.py 的区别 这2种方式主要是影响 sys.path 变量（相当于liunx中的PATH）： python -m xxx : 把执行这条命令时所在的目录添加进 sys.path 中 python xxx.py : 把xxx.py所在的目录添加进 sys.path 中 这2种方式都会把xxx.py的 __name__ 属性置为 __main__ 。 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 20:35:45 "},"python/常见问题.html":{"url":"python/常见问题.html","title":"常见问题","keywords":"","body":"常见问题 [TOC] python3 pip 安装 Pattern出错 执行jupyter contrib命令出错 sys/socket.h python3 pip 安装 Pattern出错 pip3 install Pattern出现如下错误： 主要是这一句： OSError: mysql_config not found 原因是centos需要安装mysql相关的一些依赖包，执行如下命令。然后就可以正常安装了。 yum install mysql-devel gcc gcc-devel python-devel 执行jupyter contrib命令出错 错误提示： ImportError: cannot import name 'AsyncGenerator' 解决方式：将原来的prompt-toolkit-3.0.3版本降为 2.0.10版本： pip install prompt-toolkit==2.0.10 sys/socket.h fatal error C1083: 无法打开包括文件: “sys/socket.h”: No such file .... 原因： Linux下是 sys/socket.h windows下是 winsock.h 或 winsock2.h Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 20:36:34 "},"其他/数据库.html":{"url":"其他/数据库.html","title":"数据库","keywords":"","body":"数据库 [toc] mysql 常用函数 字符集 mysql编程 SQLAlchemy 根据数据表自动生成model filter和filter_by的区别 批量插入数据 synchronize_session参数 输出方法对应的sql语句 自定义排序 反射 mysql 常用函数 字符串截取： length(str)：返回str的长度。 left(str, length)：从左边开始截取，length是截取的长度。 right(str,length)：同上，不过方向相反 substring(str, pos, [length]) 或 substr(str, pos, [length])：截取子字符串，注意 pos 是从1开始，而不是0。 locate(sub_str, str)：查找子串在str中的位置。找不到返回0。如果是substr(str, pos, [length])函数，我可能要使用查找字符串中某个字符的位置问题。所有就要使用该函数。 字符集 CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL 设置字符集为utf-8， COLLATE utf8_general_ci:数据库校对规则。该三部分分别为数据库字符集、解释不明白、区分大小写。 ci是case insensitive的缩写，意思是大小写不敏感；相对的是cs，即case sensitive。 还有一种是utf8_bin，是将字符串中的每一个字符用二进制数据存储，区分大小写。 mysql编程 drop PROCEDURE if EXISTS hggg; CREATE PROCEDURE hggg(date int, count_ int) BEGIN declare i int default 0; WHILE i 要删除procedure，不然下次再运行时会出错。 SQLAlchemy 根据数据表自动生成model Python可以利用 sqlacodegen ( sqlalchemy code generate)，根据数据库中的数据表，自动生成 SQLAlchemy model。 工具安装： pip install sqlacodegen 将db数据库中的数据表t1，t2（直接用逗号隔开），生成models.py。 sqlacodegen mysql+pymysql://user:pass@ip:port/db --tables t1,t2 > models.py # or sqlacodegen mysql+pymysql://user:pass@ip:port/db --tables t1,t2 --outfile models.py 不使用--tables选项时，默认生成数据库中所有的表。 filter和filter_by的区别 语法细节：filter用类名.属性名，比较用==，filter_by直接用属性名，比较用=。 filter不支持组合查询，只能连续调用filter来变相实现。filter_by的参数是**kwargs，直接支持组合查询。 q = sess.query(IS).filter(IS.node == node).filter(IS.password == password).all() = sess.query(IS).filter_by(node=node, password=password).all() 批量插入数据 copy：http://www.manongjc.com/detail/8-fjqznbdwcmthgru.html 方式1：每次 添加1个对象，然后commit first_time = datetime.utcnow() for i in range(10000): user = User(username=username + str(i), password=password) db.session.add(user) db.session.commit() second_time = datetime.utcnow() print((second_time - first_time).total_seconds()) # 38.14347s 方式2：bulk_save_objects second_time = datetime.utcnow() db.session.bulk_save_objects( [ User(username=username + str(i), password=password) for i in range(10000) ] ) db.session.commit() third_time = datetime.utcnow() print((third_time - second_time).total_seconds()) # 2.121589s 方式3：bulk_insert_mappings third_time = datetime.utcnow() db.session.bulk_insert_mappings(User, [ dict(username=\"NAME INSERT \" + str(i), password=password) for i in range(10000) ] ) db.session.commit() fourth_time = datetime.utcnow() print((fourth_time - third_time).total_seconds()) # 1.13548s 方式4：execute fourth_time = datetime.utcnow() db.session.execute( User.__table__.insert(), [{\"username\": 'Execute NAME ' + str(i), \"password\": password} for i in range(10000)] ) db.session.commit() five_time = datetime.utcnow() print((five_time - fourth_time).total_seconds()) # 0.888822s synchronize_session参数 该参数用于删除或者更新对象时： session.query(User).filter(User.name == 'hg').delete() session.query(User).filter(User.name == 'hg').update(values={}, synchronize_session='evaluate') 更新-值选项： False：不要同步会话。这个选项是最有效的，并且在会话过期后是可靠的，这通常发生在commit（）之后，或者显式地使用expire_all（）。在到期之前，更新的对象可能仍然保留在会话中，其属性上的值已过时，这可能导致混淆结果。 'fetch' ：在更新之前执行select查询，以查找与更新查询匹配的对象。更新的属性在匹配的对象上过期。 'evaluate' ：在Python中直接对会话中的对象评估查询条件。如果未实现对条件的评估，则会引发异常。表达式计算器目前不考虑数据库和python之间不同的字符串排序规则。 删除-值选项 False -不要同步会话。这个选项是最有效的，并且在会话过期后是可靠的，这通常发生在commit（）之后，或者显式地使用expire_all（）。在过期之前，对象可能仍然保留在会话中，而实际上已被删除，如果通过get（）或已加载的集合访问这些对象，则可能导致混淆结果。 'fetch' -在删除之前执行选择查询，以查找与删除查询匹配且需要从会话中删除的对象。匹配的对象将从会话中删除。 'evaluate' -直接在会话中的对象上用python计算查询的条件。如果未实现对条件的评估，则会引发错误。表达式计算器目前不考虑数据库和python之间不同的字符串排序规则。 输出方法对应的sql语句 方法1：直接输出 q = session.query(User).filter(User.name == 'hg') print(q) # or print(str(q)) 不足：参数不会直接被替换成'hg'，而是： WHERE user.name = %(name_1)s 方法2：可以进行参数替换 from sqlalchemy.dialects import mysql q = session.query(User).filter(User.name == 'hg') print(q.statement.compile(dialect=mysql.dialect(), compile_kwargs={\"literal_binds\": True})) 这种方式可以打印包含参数的执行语句，但是参数只包括数字和字符串等基本类型。其中dialects表示需要的数据库方言，我这里用的mysql。 自定义排序 from sqlalchemy import case, text q = session.query(User) name_case = case(value=User.name, whens={'name_1': 1, 'name_2': 2, 'name_3': 3, 'name_4': 4}) symbol_text = text(\"LEFT(symbol,1)\") q = q.order_by(name_case).order_by(symbol_text) # 不同字段同时比较 case_ = case( ( (User.name=='name_1', 1), (User.name=='name_2', 2), (User.updated==True, 3) ) ) # 注：没有出现 在排序规则里面的会排在最前面。如User.name中如果有不是name_1或name_2的字段，那么它会排在最前面。 首先通过特定的名字顺序进行排序 ，然后根据symbol属性的第一个字符进行排序。 反射 通过反射获取数据库中某张数据表的类。 from sqlalchemy import create_engine from sqlalchemy.ext.automap import automap_base engine = create_engine('mysql+pymysql://{}:{}@{}:{}/{}?charset=utf8'.format(user, pwd, ip, port, dbname), pool_size=30, pool_recycle=300, pool_pre_ping=True, echo=False) Base = automap_base() Base.prepare(engine, reflect=True) model_class = getattr(Base.classes, 'table_name') Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-28 20:54:58 "},"爬虫/爬虫.html":{"url":"爬虫/爬虫.html","title":"爬虫","keywords":"","body":"爬虫 [TOC] 主要技术 爬虫基础知识 反爬虫技术及应对手段 动态网页爬取 selenium：模拟浏览器 Splash：爬取动态页面 模拟登陆 添加cookies 模拟表单登陆 使用selenium模拟登陆 主要技术 基础技术栈 ： requests + beautifulsoup + re 高级技术栈： scrapy框架 爬虫基础知识 graph LR; start[开始链接] -->A[下载页面] A --1--> B[提取页面中的数据] A --2--> C[提取页面中的链接] C --3--> A B --> output[输出或存储] AJAX：Asynchronous JavaScript And XML。异步的javascript和xml，可以在不刷新网页的情况下更新网页的数据。 Cookies：也称为Cookie，是网站用于辨别用户身份、进行session跟踪而存储在用户本地终端上的数据。Cookie最典型的应用是判断用户是否已经登录网站、保留用户信息以简化登录手续。 request携带的是\"Cookie\"，response携带的是\"Set-Cookie\"。 如果响应头的状态码为30?，代表重定向。浏览器会根据响应头中的Location字段，再次发送GET请求。 JSON：JavaScript Object Notation，是一种轻量级的数据交换格式，是一种格式化字符串。 json与python的list或dict相互转换： import json # json -> list/dict json.loads('{\"a\":\"123\"}') # list/dict -> json json.dumps({'a':123}, indent=4) # indent添加缩进，使显示更美观 XHR：XMLHttpReques，Ajax相关的一个重要对象。 https中用到的算法： 非对称加密算法：RSA、DSA/DSS 对称加密算法：AES、RC4、3DES Hash算法：MD5、SHA1、SHA256 robots.txt：Robots Exclusion Standard，网络爬虫排除标准，网站告知网络爬虫哪些页面可以抓取，哪些不行。放在网站根目录下。 Cookie不能添加到请求头Headers中，不然只能在几个页面中得到数据，不是真正的登陆。 每个domain最多只有20条cookie，且每个cookie长度不超过4kB，否则会被截断。 数据来源：大部分数据来源于Doc、XHR、JS标签。 Requests的GET和POST方法的请求参数分别是params和data。 如果解码出错，可以使用errors选项忽略掉： # xx是一个bytes xx.decode(encoding=response.encoding, errors='ignore') 不分析url，直接爬取手机上的数据：使用UiAutomator操作Android手机（需要安装Android SDK、JRE），然后使用python来操作UI Automator，从而控制手机。将手机上的内容转换成xml形式，然后进行抓取。 模拟人操作手机屏幕的任何行为，并直接读取屏幕上的文字。 需要安装第三方库： pip install uiautomator 注：UI Automator Viewer与python uiautomator 不能同时使用。 反爬虫技术及应对手段 用户请求的Headers：对Headers的User-Agent进行检测，或者对Referer进行检测（一些资源网站的防盗链就是检测Referer） 用户操作网络行为：通过检测用户行为来判断用户行为是否合规。 同一IP短时间多次访问同一页面：使用IP代理，可以在IP代理平台上通过API接口获得，每请求几次就更换一个IP。 同一账户短时间内多次进行相同操作：每次请求间隔几秒后再发送下一个请求，可以在代码中加一个延时功能。 网站目录数据加载方式（采用异步加载）：针对Ajax请求，分析具体的请求参数和响应的数据结构及其含义，在爬虫中直接模拟Ajax请求。 数据加密：某些网站可能对Ajax的请求参数进行加密 加密代码一般是使用javascript实现的，可以自行分析加密方式。 使用Selenium+chromedriver（自动化测试技术），调用浏览器内核，利用Selenium模拟人为操作网页，并触发网页中的js脚本。 验证码识别：通过第三方平台处理，或OCR技术识别。 动态网页爬取 主要内容：抓取动态网页，执行相关的javascript代码，使用鼠标进行点击等 技术选择： selenium：适合用来加载动态页面，进行比较复杂的登陆操作（各种类型的验证码）。 splash：用于加载动态页面（执行js）。 selenium：模拟浏览器 selenium是一个网页自动化测试工具，可以通过代码来操作网页上的各个元素。 python使用： pip install selenium 下载浏览器驱动程序： Chrome：http://chromedriver.chromium.org/downloads（这个需要vpn），http://chromedriver.storage.googleapis.com/index.html 。 Firefox： PhantomJS：https://phantomjs.org/download.html。 PhantomJS是没有无界面的，其他的2个有界面（也可以使用headless参数屏蔽页面）。 注：驱动要和本机上装的相应浏览器的版本保持一致，不然会出错。 chromedriver/firfoxdriver from selenium import webdriver from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.common.by import By from selenium.webdriver.support import expected_conditions chrome = webdriver.Chrome(executable_path=chrome_path) # chrome_path指定驱动路径 chrome.get(login_url) WebDriverWait(chrome, 5).until( expected_conditions.title_is('登录 - 新浪微博'), '加载登陆页面失败' ) ? scrapy程序采用中间件MobilesDownloaderMiddleware实现Chrome浏览器的嵌入，当一个Request申请时，如果不是图片申请就使用Chrome浏览器获取网页的Response，以保证js的执行。否则直接使用默认方法得到response。 Splash：爬取动态页面 splash文档：https://splash.readthedocs.io/en/latest/index.html github地址：https://github.com/scrapy-plugins/scrapy-splash splash是一个JavaScript渲染引擎。使用WebKit开发的轻量级无界面浏览器，提供基于HTTP接口的JavaScript渲染服务。 splash安装 在linux上，使用docker运行splash： docker pull scrapinghub/splash docker run -p 8050:8050 -p 5023:5023 scrapinghub/splash docker run -p 8050:8050 scrapinghub/splash 使用requests进行测试： import requests from scrapy.selector import Selector host = 1.1.1.1 # 你docker（或splash）的安装地址 splash_url = 'http://' + host +' :8050/render.html' args = {'url':'http://quotes.toscrape.com/js','timeout':10,'image':0} response = requests.get(splash_url,params = args) sel = Selector(response) sel.css('div.quote span.text::text').extract() 最后输出不为空，则说明安装成功。 splash服务： render.html：提供JavaScript页面渲染服务。 execute：执行用户自定义的渲染脚本（lua），利用该端点可以在页面中执行JavaScript代码。 render.html 服务端点 render.html 请求地址 http://localhost:8050/render.html 请求方式 GET/POST 返回类型 html 常用参数： args = { 'url':'http://quotes.toscrape.com/js', # 需要渲染的js页面，必选 'timeout':10, # 渲染页面超时时间，float 'wait':2, # 等待页面渲染的时间，float 'proxy':'http://u:passwd@host:port', # 代理服务器地址 'image':0, # 0为不下载图片，1为下载图片（默认） 'js_source':'', # 用户自定义的js代码，在页面渲染前执行 } execute端点 可以将execute端点的服务看做一个可用lua语言编程的浏览器，功能类似PhantomJS。使用lua脚本来模拟浏览器行为。 服务端点 execute 请求地址 http://localhost:8050/execute 请求方式 POST 返回类型 自定义 常用参数： args = { 'lua_source':'xxxx', # 用户自定义的lua脚本，必选 'timeout':10, # 渲染页面超时时间，float 'proxy':'http://u:passwd@host:port', # 代理服务器地址 } lua脚本必须包含一个main函数作为程序入口，main函数会传入一个splash对象（lua中的对象），用户可以调用该对象的方法来操作splash。 lua函数： function main(splash) splash:go(\"http://example.com\") -- 打开页面 splash:wait(0.5) -- 等待加载 local title = splash:evaljs(\"document.title\") -- 执行js代码 return {title=title} -- 返回lua中的表，会被编码成json串。 end 常用函数： splash:url()：返回当前页面的url。 splash:html()：返回当前页面的html文本。 splash:get_cookies()：获取全部Cookie信息。 在scrapy中使用splash 首先需要安装scrapy-splash： pip install scrapy-splash 相关配置： # setting.py SPLASH_URL = 'http://127.0.0.1:8050/' # 开启下载中间件，并调整HttpCompressionMiddleware的次序 DOWNLOADER_MIDDLEWARES = { # Engine side 'scrapy_splash.SplashCookiesMiddleware': 723, 'scrapy_splash.SplashMiddleware': 725, 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810, # Downloader side } # 去重过滤器 DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter' # 支持cache_args（可选） SPIDER_MIDDLEWARES = { 'scrapy_splash.SplashDeduplicateArgsMiddleware': 100, } HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage' 在scrapy_splash中有一个SplashRequest类，用户只需使用SplashRequst来替代Request发送请求。 在爬虫中调用： from scrapy_splash import SplashRequst yield SplashRequest(url,self.parse,endpoint='render.html', args={} ) 模拟登陆 添加cookies 1. 从浏览器中复制请求头 在登陆某网站后，复制该页面的Request中Headers的Cookie。将其添加到请求头中。 有一种方法，直接copy请求头，然后将其附着到session上，这样只能访问你copy的那个网页，cookies并不会真正的被设置。 2. 使用browser_cookie3从浏览器中获得cookie python库browser_cookie3可以获取chrome或firefox浏览器中的Cookie。 import browser_cookie3 # 返回http.cookiejar.CookieJar browser_cookie3.chrome(domain_name=domain) 获取一个已经登录网站的cookie： import requests import browser_cookie3 # 返回一个 requests.cookies.RequestsCookieJar def get_cookiejar_from_browser(domain): cj = browser_cookie3.chrome(domain_name=domain) cookiejar = requests.cookies.RequestsCookieJar() for c in cj: cookiejar.set(c.name, c.value, domain=c.domain, path=c.path) return cookiejar 模拟表单登陆 适合没有或只有简单验证手段的网站。如简单的数字验证码，可以调用ocr模块进行识别。 表单中可能会有隐藏项。 可以安装loginform来快速构建表单数据。 from loginform import fill_login_form formdata, login_url, method = fill_login_form(login_url, response.text, user, passwd) fill_login_form 返回包含3个元素的元组，分别是所有表单项组成的列表（每隔表单项是一个(x,y)类型的元组），登陆提交表单的地址，提交表单的方法。 OCR识别验证码 使用orc识别验证码，需要现在对应电脑上安装orc软件。 from PIL import Image # PIL通过pip install pillow安装 import pytesseract from io import BytesIO img = Image.open('code.png') # img = Image.open(BytesIO(response.body)) # 使用ByteIO(bytes)转换 img = img.convert('L') # 转换为黑白图 res = pytesseract.image_to_string(img) img.close() 网络平台识别验证码 可以在阿里云市场找到验证码识别平台。购买，调用相关接口，获取识别结果。 人工识别验证码 将验证码图片显示出来，然后人工输入验证码。 使用selenium模拟登陆 适用于验证码比较复杂的。 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"爬虫/bs4.html":{"url":"爬虫/bs4.html","title":"bs4","keywords":"","body":"BeautifulSoup [TOC] from bs4 import BeautifulSoup soup = BeautifulSoup('data', 'html.parser') print(soup.prettify()) # 友好显示, 输出换行与前导空格 主要有4个解析器： bs4的html解析器：html.parser，需安装bs4 lxml的html解析器：lxml，需安装lxml lxml的xml解析器：xml，需安装lxml html5lib的解析器：html5lib，需安装html5lib 下行遍历： tag.contents：所有儿子节点组成的列表。 tag.children：子节点的迭代类型。 tag.descendants：子孙节点的迭代类型。 上行遍历： tag. parent：节点的父标签。 tag.parents：节点先辈标签的迭代类型。 平行遍历： tag.next_sibling：按照html文本顺序，返回下一个平行节点标签。 tag.next_siblings：返回所有后续的平行节点标签。 tag.previous_sibling tag.previous_siblings 提取信息： tag.find_all(name, attrs, recursive, string, **kwargs) # 返回一个bs4.element.Tag的列表 参数解释： name：可以是关于标签的列表、正则表达式、函数（对tag进行过滤） string：<>…中字符串区域的检索字符串（就是只检索非属性字符串） tag(..) 等价于tag.find_all(..) bs可以使用类似CSS的语法，tag.select(css)，tag是html中的一个element节点元素。 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"爬虫/requests.html":{"url":"爬虫/requests.html","title":"requests","keywords":"","body":"requests 模块 [TOC] requests主要有6个方法：get，head，put（覆盖），post（追加），patch（局部修改），delete。 Request requests.get(url, params=None, **kwargs) kwargs参数： params : 字典或字节序列，作为参数增加到url中 data : 字典、字节序列或文件对象，作为Request的内容 json : JSON格式的数据，作为Request的内容 headers : 字典，HTTP定制头 cookies : 字典或CookieJar，Request中的cookie auth : 元组，支持HTTP认证功能 files : 字典类型，传输文件 timeout : 设定超时时间，秒为单位 proxies : 字典类型，设定访问代理服务器，可以增加登录认证 allow_redirects : True/False，默认为True，重定向开关 stream : True/False，默认为True，获取内容立即下载开关 verify : True/False，默认为True，认证SSL证书开关 cert : 本地SSL证书路径 requests设置代理 proxies = { \"http\": \"http://10.10.1.10:3128\", \"https\": \"http://10.10.1.10:1080\", } requests.get(\"https://www.baidu.com/\", proxies=proxies) https 证书验证 url = 'https://kyfw.12306.cn/otn/leftTicket/init' # 关闭证书验证 r = requests.get(url, verify=False) # 开启证书验证 r = requests.get(url, verify=True) r = requests.get(url, verify= '/path/to/certfile') # ?设置证书所在路径 Response（requests.models.Response） 常用属性： status_code：状态码 headers：一个字典 encoding：从http header中获取，可能不准。 apparent_encoding：从内容中获取编码格式，比较耗时。 content：http响应内容的二进制格式。 text：响应内容的字符串格式。 常用方法： res.raise_for_status：如果状态码不是200，就产生异常requests.HTTPError。 json：返回json响应内容。 requests爬取网页的一般框架 爬取网页 import requests def get_html(url): try: r = requests.get(url, timeout=30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return 'error' 爬取图片 def get_image(url, image_path): r = reqquests.get(url) with open(image_path, 'wb') as f: f.write(r.content) 加载图片： from PIL import Image from io import BytesIO i = Image.open(BytesIO(r.content)) Session Session对象让你能够跨请求保持某些参数。它也会在同一个 Session 实例发出的所有请求之间保持 cookie。 注：如果把cookies或headers放进请求参数中，在session进行第一次请求后，这些参数不会自动添加到后续的请求中。因此，为了是参数在session中共用，应该将其添加到session上。 session = requests.session() session.cookies = get_cookiejar() session.headers = {} session.get('https://www.baidu.com') Cookie cookies需要是 requests.cookies.RequestsCookieJar，是 http.cookiejar.CookieJar 的子类。二者的Cookie都是http.cookiejar.Cookie。 使用cookie： import requests url = 'https://movie.douban.com/' r = requests.get(url) mycookies = r.cookies # RequestsCookieJar对象 print(mycookies) # RequestsCookieJar转换字典 cookies_dict = requests.utils.dict_from_cookiejar(mycookies) print(cookies_dict) # 字典转换RequestsCookieJar cookies_jar = requests.utils.cookiejar_from_dict(cookies_dict, cookiejar=None, overwrite=True) print(cookies_jar) # 在RequestsCookieJar对象中添加Cookies字典 requests.utils.add_dict_to_cookiejar(mycookies, cookies_dict) Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"爬虫/scrapy.html":{"url":"爬虫/scrapy.html","title":"scrapy","keywords":"","body":"scrapy [TOC] 一些小知识点 scrapy命令 查看scrapy相关信息 创建爬虫 启动爬虫 为爬虫添加参数 为项目添加参数 使用交互式命令进行调试 替换start_urls 启动爬虫，但只爬取部分数据 暂停scrapy爬虫执行 使用脚本启动scrapy爬虫 导出数据 使用模板创建爬虫 相关类说明 1. Request（scrapy.http.request.Request） 2. Response（scrapy.http.response.Response） 3. 自己编写的Spider（继承scrapy.Spider） 4. 其他 数据提取 1. xpath选择器 2. css选择器 3. re 4. LinkExtractor：只提取链接 item封装数据 ItemLoader pipeline 1. FilesPipeline 2. ImagesPipeline extension 1. Exporter 模拟登陆 1. 构造表单 2. 使用Cookie 数据存储 1. SQLite 2. mysql 3. mongodb 4. Redis 5. 异步访问数据库 网络代理 中间件 1. 下载中间件 2. 爬虫中间件 settings custom_settings settings的使用技巧 分布式爬取 scrapy使用注释进行调试 爬虫部署 telnet 一些小知识点 Scrapy不是一个函数功能库，而是一个爬虫框架。爬虫框架是实现爬虫功能的一个软件结构和功能组件集合。 爬虫框架是一个半成品，需要用户实现部分代码。 scrapy是基于事件的，能在打开上千个链接的同时（并行运行多个请求），通过平稳的操作拆分吞吐量的延迟。 从输出日志出查看爬虫运行过程： 准备启动爬虫 -> 加载setting -> 启动extension -> 启动下载中间件 -> 启动爬虫中间件 -> 启动pipeline -> 爬虫已经启动（spider_opened信号） -> 爬取信息 -> 爬取完成，开始关闭爬虫 -> 显示统计信息 -> 爬虫关闭 日志等级：DEBUG、INFO、WARNING、ERROR、CRITICAL、SILENT（不记录任何日志） scrapy框架结构： item pipeline后面可以接exporter（一种extension），将item以某种数据格式写入文件。 需要自己编写的：spider、pipeline、settings、middleware（根据需要）。 scrapy命令 查看scrapy相关信息 安装scrapy后，查看版本号： #!/bin/python3 import scrapy scrapy.version_info 可以使用如下命令进行命令查看： scrapy -h scrapy -h scrapy [options] [args] 创建爬虫 scrapy genspider [mySpider] [domain] 可以使用参数 --template=TEMPLATE, -t TEMPLATE 指定创建爬虫使用的模板文件。 启动爬虫 scrapy crawl [spider_name] # spider_name是某个py文件的名字 为爬虫添加参数 运行爬虫时，可以使用-a添加参数。传进去的都是字符串。如下所示： scrapy crawl [mySpider] -a x=123 -a y=hg 可以使用以下方法获取： # 在mySpider类中 x = getattr(self, 'x', 'default') x = self.x # 如果没有传x参数，该方法或出错 为项目添加参数 scrapy crawl [mySpider] -s SOME_ARG=xxx 使用交互式命令进行调试 scrapy shell [url] --nolog # 不要log是为了好看 使用view(response)在浏览器中显示response中的页面（可能和在网上看到的不一样，一个是scrapy爬虫下载的页面，一个是浏览器下载的页面）。 替换start_urls 指定一个爬虫，爬取给定url的信息（会替换掉爬虫中的 start_urls）： scrapy parse --spider= 这是一个相当方便的调试命令。 启动爬虫，但只爬取部分数据 若是要测试爬虫，而爬虫会爬取上万个页面，可以使用如下参数限定Item或page数量： scrapy crawl -s CLOSESPIDER_ITEMCOUNT=90 scrapy crawl -s CLOSESPIDER_PAGECOUNT=9 暂停scrapy爬虫执行 通过增加参数JOBDIR来进行： 在启用爬虫时，即在命令行中加入参数： scrapy crawl [mySpider] -s JOBDIR=xxx/001 在设置文件settings.py添加参数 # settings.py JOBDIR='xxx/001' 在添加完参数后，运行爬虫。在爬虫运行时，按ctrl+c（只按一次）停止爬虫，停止后就会在目录xxx/001下生成相关文件，用来保存相关信息。 注意：这里只能按一次ctrl+c，如果按了两次就表示强制退出了。 生成的xxx/001目录在项目的根目录下（即与scrapy.cfg文件在同一目录）。 下次我们想接着上次爬，就可以用同样的命令启动爬虫，代表我们使用xxx/001文件夹下的状态信息。 使用脚本启动scrapy爬虫 #!/bin/python3 from scrapy import cmdline cmdline.execute('scrapy crawl [spider_name]'.split()) 导出数据 -o：指定导出文件路径。可以使用后缀名来指定导出格式。 -t：指定导出数据格式。 scrapy crawl [mySpider] -t csv -o 'export_data/%(name)s_%(time)s.csv' %(name)s：Spider的名字。 %(time)s：文件创建时间。 使用模板创建爬虫 使用指定的模板创建爬虫（默认是basic） scrapy genspider -t basic|crawl|csvfeed|xmlfeed [spider_name] [domain] 相关类说明 1. Request（scrapy.http.request.Request） Request(url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None) 参数解释： callback：Callable类型，Request对象请求的页面下载完成后，由该参数指定的函数解析页面内容。 meta：元数据字典，dict类型，用于在组件之间传递数据。 dont_filter：如果对同一url进行多次请求，不对重复请求进行过滤。如果爬取的页面内容会随着时间变化，应该将其置为True。 errback：Callable类型，请求出现异常或者http错误时的回调函数。 request常用属性： url、method、headers、body（bytes或string类型）、meta（字典） 2. Response（scrapy.http.response.Response） Response(url, status=200, headers=None, body=b'', flags=None, request=None) Response只是一个基类，有如下子类：TextResponse、HtmlResponse、XmlResponse。下载器会根据HTTP响应头的Conten-Type创建某个具体的Response对象。 response常用属性： url、status、headers（一个类似dict的对象，有get和getlist方法） encoding：http响应正文的编码。 body：http响应正文，bytes类型。 text：http响应正文，string类型。由body属性通过encoding属性解码得到。 request：产生该http响应的request对象。 meta：即response.request.meta。 selector：用于从Response中提取数据。 response常用方法： urljoin(relative_url)：将相对链接url构造成绝对链接。 xpath（query）或css（query）：实际是response.selector.xxx(query)，用于在Response中提取数据。提取出来的是Selector。 re（正则表达式）：用于在Response中提取数据。提取出来的是字符串。 3. 自己编写的Spider（继承scrapy.Spider） 常用属性： name：spider的唯一标识，各个spider之间的name不一样。 start_urls：list，起始爬取页面。调用parse()解析函数。 logger：自带的日志。 settings：访问settings。 常用方法： parse：页面解析函数，在Request不指定callback参数时，默认使用该方法。 start_requests：在没设置start_urls时，使用该方法自己爬取起始页（自己构造Request对象）。 4. 其他 在CrawlSpider（使用 -t crawl 生成）中，使用Rule来跟踪链接，以实现爬虫的双向爬取（对一个页面进行横向和纵向爬取）。 from scrapy.spiders import Rule rule = Rule(link_extractor, callback = None, cb_kwargs = None, follow = None, process_links = None, process_request = None ) \"\"\" link_extractor：用于抽取链接 callback：回调函数（即处理response的函数），是一个字符串，而不是方法引用。 cb_kwargs：是一个包含要传递给回调函数的关键字参数的dict。 follow：如果没有设置callback，rule会继续跟踪抽取到的url，并对其抽取链接。如果设置了callback后，仍想跟踪链接，可以将follow设为True，或者在callback中使用yield或return返回它们。 \"\"\" 数据提取 HTTP常用的文本解析模块： BeautifulSoup：API简单易用，但解析速度较慢。 lxml：由C语言编写的xml解析库，速度快，但API复杂。 scrapy的Selector类，是基于lxml库构建的，并简化了API接口。 Selector内容提取的方法： extract() re() extract_first()：SelectorList专有。 re_first()：SelectorList专有。 可以借助谷歌浏览器快速获得某个节点的xpath或css路径： F12 -> 点击某个节点 -> 单击右键 -> Copy -> Copyselector（css）或者 Copy xpath。 此外，还可以在console中使用表达式 $x('xpath表达式') 来获取相应节点： $x('//*[@id=\"content_views\"]/pre[15]') 1. xpath选择器 XPath：XML路径语言（XML Path Language）。 xpath基础语法： 表达式 描述 / 用在xpath表达式的最前面，表示文档的根（不是一个节点）。 //xx 用在xpath表达式的最前面，表示文档中所有的xx节点。 用在xpath表达式的最前面，表示当前节点的所有子节点。 yy//xx yy节点的子孙中，所有的xx节点 . 当前节点，用于描述相对路径。 .. 父节点，用于描述相对路径。 xx/* xx节点的所有子节点 xx/*/yy 选中xx的孙子节点中，所有的yy节点 xx/@attr 选中xx节点的attr属性。\"a/@href\"：选中a标签的href属性。 xx/@* 选中xx节点的所有属性 xx/text() 选中xx节点中的文本信息 node[谓词] 用来查找满足谓词条件的节点，如div[@id=\"xx\"]，表示id为\"xx\"的div节点 常用谓语函数： 谓语 含义 数字>0 用来选第几个节点。如xx[num]，表示选中第num个节点xx。 xx[last()] 选中最后一个xx节点。 xx[position() 选中前2个xx节点。 xx[@attr] 选中含有attr属性的xx节点。 xx[@attr=”hg“] 选中含有attr属性为”hg“的xx节点。 xx[contains(@href, ”hg“)] 选中href属性中包含有\"hg\"的xx节点。即”hg“是@attr的子串。'div[contains(text(), ”hg”)]' xx[starts-with(@id, ”hg“)] 选中xx节点，该节点的attr属性以”hg“开头。 and 如 ‘div[contains(@id, ”hg”) and contains(@href, ”www”)]’ string(a/b/c) 返回c节点（只找第一个c节点）下所有子孙节点的文本信息的组合（即将字符串数组变成一个字符串） string详解： s = Selector() a = s.xpath( 'string(/html/body/a)' ).extract_first() b = s.xpath('/html/body/a//text()').extract() c = ''.join(b) a==c # True 2. css选择器 css即层叠样式表。 当使用Selector对象的css方法时，在内部会将css表达式翻译成xpath表达式，然后调用xpath方法。 表达式 描述 * 选中所有节点 xx,yy 选中所有的xx节点和yy节点。 xx yy 选中xx节点下的所有子孙节点yy。二者之间是空格。 xx>yy 选中xx节点下的所有子节点yy xx+yy 选中xx节点的所有兄弟节点yy ._class 选中class属性为_class的节点。如 'div.info' 表示选中class=info的div节点。 #_id 选中id为_id的节点。如 'div#main' 表示id=main的div节点。 [attr] 选中包含属性attr的节点。 [attr=value] 选中属性attr为value的节点 [attr*=value] 选中属性attr的值包含value的节点，即value是attr属性的子串。 a[href^=\"http\"] 选取所有href属性以http开头的a元素。 a[href$=\".jpg\"] 选取所有href属性以.jpg结尾的a元素。 xx:nth-child(n) 选中第n个xx节点 xx:nth-last-child(n) 选中倒数第n个xx节点 xx:first-child 第一个xx节点 xx:last-child 最后一个xx节点 xx:empty 没有子节点的xx xx::text xx节点的文本节点 xx::attr(href) 选取xx节点的href属性 div:not(#main) 选取所有id不为main的div元素 3. re 使用正则表达式来提取文本。 # 提取所有单词 response.selector.re(r'\\w+') r'.' 是贪婪匹配（最长匹配），r'.\\?' 是非贪婪匹配（最短匹配）。 '.' 能匹配除了换行符的任一字符。 4. LinkExtractor：只提取链接 LinkExtractor是一个专门用来提取链接的类，适用于提取大量链接或提取规则比较复杂的情况。 用来提取页面中的链接。 from scrapy.linkextractors import LinkExtractor # 实际是LxmlLinkExtractor le = LinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None, deny_extensions=None, restrict_css=()) links = le.extract_links(response) # 提取出来的是scrapy.link.Link的列表。得到的link是绝对地址。 links = [link.url for link in links]# 获取链接 ？使用这个提取出的链接在末尾可能有问题，如本来是\"baidu.com\"，结果变成了\"baidu.com/\"。 参数解释： allow：一个正则表达式或一个正则表达式列表。？正则不是完全匹配，是有部分匹配就OK了。 deny：同allow，只是意思相反。 allow_domain：一个域名或列表，提取指定域名的link。 restrict_xpaths：一个xpath表达式或列表。只能提取该表达式选中区域下的link。 tags：用于提取指定标签中的链接。 attrs：只能提取tags中的attrs标签。 process_value：接受一个回调函数，对提取的链接（一个字符串）进行处理。如果丢弃某个链接，返回None。 item封装数据 item是一个类似dict的对象，支持字典接口。定义如下： import scrapy class MyItem(scrapy.Item): name = scrapy.Field() age = scrapy.Field() 在对item对象赋值时，如果字段名没有预先定义，将会抛出异常（防止粗心将字段名写错）。 但是，item可以使用fields属性动态添加字段： item = Item() item.fields['name'] = Field() 如果想要传递额外信息给处理数据的某个组件，可以使用Field的元数据： class MyItem(scrapy.Item): x = scrapy.Field(len=20, serializer=lambda x: x) > it = MyItem() > it.fields Out: {'x': {'len': 20, 'serializer': >}} > it.fields['x'] Out: {'len': 20, 'serializer': >} > type(it.fields['x']) Out: scrapy.item.Field 元数据可以用来对Item的某个属性进行限制：如x怎么进行序列化。 item除了记录用户想要的信息外，还可以添加一些额外信息，帮助我们调试爬虫，比如何地（url）、何时（time）、使用那个爬虫（spider）抓取的数据。 ItemLoader 在爬虫中，构造Item除了先生成一个对象，然后填充相应属性外，还可以使用ItemLoader来完成。 from scrapy.loader import ItemLoader from scrapy.loader.processors import MapCompose, Join l = ItemLoader(item=myItem(), response=response) # l = ItemLoader(item=myItem(), selector=response.selector) l.add_xpath('name', '//*[@id=\"name\"][1]/text()', MapCompose(str.strip, str.title), Join(separator='|')) # l.add_xpath(self, field_name, xpath, *processors, **kw) # l.add_css(...) # l.add_value('url', response.url) item = l.load_item() MapCompose可以填入函数链，用于对提取出来的每一个元素进行清洗。 pipeline pipeline的作用： 清洗数据 验证数据的有效性 过滤掉重复的数据 将数据保存到文件或数据库 pipeline定义： class MyPipeline(object) def __init__(self): # 可选实现，做参数初始化等 pass def process_item(self, item, spider): # item是爬取到的数据， spider是爬取到该数据的Spider对象。 if True: return item else: raise scrapy.exceptions.Dropitem() def open_spider(self, spider): # 可选实现，当spider被开启时，这个方法被调用。 pass def close_spider(self, spider): # 可选实现，当spider被关闭时，这个方法被调用 pass @classmethod def from_crawler(cls, crawler): return cls() 主要方法： process_item：主要方法，对item进行处理。如果item不需要继续在流水线上进行处理，可以抛出DropItem异常（scrapy.exceptions.Dropitem)，该item就会被抛弃。 fromcrawler(cls, crawler)：通常在该方法中，通过crawler.settings来读取相关配置信息，然后根据配置调用_init方法创建Item Pipeline对象。 _init_(self)：可以用于初始化某些数据。比如创建一个集合，用来保存所有经过该流水线的item，然后用于查重。 过滤重复数据：在初始化时，创建一个set。然后在处理item时判断该项是否在set中。 1. FilesPipeline 配置： # settings.py ITEM_PIPELINES = {'scrapy.pipelines.files.FilesPipeline': 1} FILES_STORE = '/path/to/valid/dir' # 相对于项目的根目录（scrapy.cfg所在目录） 下载过程： 在item中查找\"file_urls\"字段（是一个列表）。 根据\"file_urls\"字段下载相应的文件。 向item添加字段'files'，存放下载结果。主要有： Path：文件下载到本地的路径（相对于\"FILES_STORE\"的相对路径）。 Checksum：文件的校验和。 url：文件的url地址。 下载的文件名为： # SHA1_HASH_VALUE为下载文件url的sha1散列值。是为了防止重名文件覆盖。 [FILES_STORE]/full/[SHA1_HASH_VALUE].[format] 重载命名方法： from urllib.parse import urlparse import os class MyPipeline(FilesPipeline): def file_path(self, request, response=None, info=None): # 根据request的相关信息，返回一个文件名（字符串） # 传进来的request可能是一个url if not isinstance(request, Request): url = request else: url = request.url path = urlparse(url).path # 得到url中的路径部分(除去域名、参数等) dir = os.path.dirname(path) name = os.path.basename(path) return 'xxx' 2. ImagesPipeline ImagesPipeline是FilesPipeline的子类。 使用上大致相似，只是item字段由\"file_urls\"、\"files\"变成了\"image_urls\"、\"images\"。 配置： # settings.py ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1} IMAGES_STORE = '/path/to/valid/dir' # 生成缩略图 THUMBS = { 'small':(50,50), 'big':(270,270), } # 过滤掉长宽不符合要求的图片 IMAGES_MIN_WIDTH = 110 IMAGES_MAX_WIDTH = 110 extension 1. Exporter 已有实现： json json lines：jsonlines或jl。 csv xml pickle marshal 前4种为常用格式，后2中为python特有。 配置： # 位于scrapy.settings.default_settings模块 FEED_EXPORTERS_BASE = { 'json': 'scrapy.exporters.JsonItemExporter', # ... } # settings.py FEED_EXPORTERS = { 'myjson': 'MyItemExporter', } 如果自己实现的话，需要继承BaseItemExporter： from scrapy.exporters import BaseItemExporter def MyItemExporter(BaseItemExporter): def export_item(self, item): # 负责导出数据 pass def start_exporting(self): # 在导出开始时被调用，用于某些初始化工作 pass def finish_exporting(self): pass 模拟登陆 1. 构造表单 scrapy提供了一个FormRequest类（Request的子类），用于构造含有表单的请求。 构造FormRequest： from scrapy.http import FormRequest fd = { 'username':'xxx', 'password':'xxx', } request = FormRequest.from_response(response, formdata=fd) # 隐藏的表单项会自动填充。 2. 使用Cookie 详情见Cookie中间件 数据存储 主要有SQLite、MySQL、MongoDB、Redis。 1. SQLite 在python3中： import sqlite3 conn = sqlite3.connect('example.db') cur = conn.cursor() cur.execute('insert into person values(%s,%s)', (name, age)) conn.commit() conn.close() 2. mysql python3有pymysql、mysqlclient包。 在python2中使用MySQLdb访问数据库，在python中使用mysqlclient作为替代，提供几乎完全相同的接口。 import MySQLdb # 在python3中，实际安装的是mysqlclient包 import pymysql conn = sqlite3.connect(host='localhost', db='example', user='root', passwd='xxx', charset='utf8') \"\"\" conn = pymysql.connect(host='localhost', port=3306, db='example', user='root',passwd='xxx', charset='utf8') \"\"\" cur = conn.cursor() cur.execute('[SQL statement]') conn.commit() conn.close() 3. mongodb mongodb是一个文档数据库，数据以键值对进行存储。 数据组织：库（database） -> 集合（collections） -> 文档（document） windows上可以使用mongodb的管理软件RoboMongo。 默认情况下，mongodb只允许本机访问。如需远程访问，需修改配置文件： # mongod.conf # bindIp: 127.0.0.1 bindIp: 0.0.0.0 如非必要，别远程访问mongodb。 from pymongo import MongoClient uri = \"mongodb://[user]:[passwd]@[localhost]:27017\" client = MongoClient(\"mongodb://localhost:27017\") db = client['example_db'] db['example_collection'].insert_one({'name':'xx','age':18}) client.close() 4. Redis redis是一个基于内存的非关系型数据库。redis以键值对（key-value）存储数据。 默认情况下，redis只能被本机访问，如果需要远程访问，需要对配置文件进行修改： # /etc/redis.conf # bind 127.0.0.1 bind 0.0.0.0 # protected-mode yes protected-mode no # requirepass foobared requirepass [my_password] 注：远程访问redis风险极大，请谨慎使用。 键值对中的值主要有5种类型： 字符串（string）：可以存储字符串、整数、浮点数（数字也是字符串）。 哈希（hash）：用于存储多个键值对，其中的键值都是字符串。 列表（list） 集合（set） 有序集合（zset） from redis import StrictRedis conn = StrictRedis(host='127.0.0.1', port=6379, db=redis_db_index) conn.hset('person', 'xiaoming', {'name':'xx','age':18}) conn.connection_pool.disconnect() 5. 异步访问数据库 Twisted提供了以异步方式多线程访问数据库的模块adbapi。使用该模块可以显著提高程序访问数据库的效率。 from twisted.enterprise import adbapi # adbapi只是提供了一个异步访问数据库的编程框架，内部依旧使用MySQLdb、sqlite3这样的库访问数据库 dbpool = adbapi.ConnectionPool('MySQLdb', host='localhost', database='example', user='root', passwd='xxx', charset='utf8') # 该函数将被异步调用。tx是一个Transaction对象，其接口与cursor类似。在该方法执行完后，会自动commit def insert_db(tx, item): name, age = item tx.execute('insert into person values(%s,%s)', (name, age)) dbpool.runInteraction(insert_db, (name, age)) dpool.close() 网络代理 爬虫使用代理的原因： 直接爬取速度太慢，使用代理提高爬取速度。 网站对用户进行访问限速，爬取过快会封ip。 某些网站被墙，不能直接访问。 scrapy中见HTTP代理中间件 中间件 中间件：对经过中间件的对象进行修改、丢弃、新增操作。 scrapy中间件中，处理request请求的函数，返回不同的值： None：将请求交给后续的中间件进行处理。 Request：将请求交给调度器重新调度，并终止后续中间件的执行； Response：终止后续中间件及下载器的执行，直接将Response交给引擎。 Exception：抛出异常。 scrapy中间件中，处理response响应的函数，返回不同的值： Request：终止后续中间件的执行，将请求重新交给调度器进行调度。 Response：继续执行后续的中间件。 Exception：抛出异常。 1. 下载中间件 用途： 代理中间件 User-Agent中间件 Cookies中间件 集成selenium的中间件 对请求失败进行重试的中间件 对请求异常进行处理的中间件 1.1 Cookie中间件 scrapy.http.cookies.CookieJar # 1 http.cookiejar.CookieJar # 2 http.cookiejar.Cookie # 3 1是对2的封装，使用1.jar就可以的到2。而2是由多个Cookie（3）组成的，可以使用循环遍历Cookie。 Cookie有多个属性，如name、value、domain、path、expires等。 Request通过meta属性中的'cookiejar'指定使用CookiesMiddleware中的哪一个CookieJar。 1.2 HTTP代理中间件 HttpProxyMiddleware是默认开启的，它会从系统环境变量中搜索当前系统代理（名字格式为xxx_proxy的环境变量），作为Scrapy爬虫的代理。 scrapy中设置代理，就是将代理服务器的 url 填写到 request.meta['proxy'] 中。 HttpProxyMiddleware对于一个协议（如http），只能设置一个代理。为了使用多个代理，可以将代理服务器的 url 添加进request请求的meta['proxy'] 中，从而实现多个代理。这样做，如果代理存在验证环节的话，还需要自己手动将 \"user:passwd\" 进行编码，放入请求头的 Proxy-Authorization 中。可以参考源码的实现。 假设我们在linux上搭建好了代理服务器，设置环境变量： # 这样只是临时设置，在shell关闭时失效。 export http_proxy=\"http://123.123.123.123:1234\" # http代理 export https_proxy=\"http://123.123.123.123:12345\" # https代理 # 带验证 export http_proxy=\"http://user:passwd@123.123.123.123:1234\" # http代理 export https_proxy=\"http://user:passwd@123.123.123.123:12345\" # https代理 可以使用http://httpbin.org/提供的服务来窥视我们发送的请求，如源ip地址、请求头部、Cookie等信息。可以用来验证代理是否正常工作。 访问 http://httpbin.org/ip ，将返回一个包含请求源IP地址信息的json串。 获取免费代理的网站： http://proxy-list.org/ （国外） https://free-proxy-list.org/ （国外） http://www.xicidaili.com http://www.proxy360.cn http://www.kuaidaili.com 2. 爬虫中间件 作用： 处理爬虫本身的异常 settings 官方文档中scrapy中settings参数有四个级别： 命令行选项(Command line Options)（最高优先级）：scrapy crawl somespider -s LOG_FILE=t.log 项目设定模块(Project settings module)：custom_settings 命令默认设定模块(Default settings per-command)：项目目录下的settings文件。 全局默认设定(Default global settings) (最低优先级) custom_settings custom_settings可以理解为spider的个性设置，通常我们在一个项目目录下会有很多个spider，但是只有一个settings.py全局配置文件，为了让不同的spider应用不同的设置，我们可以在spider代码中加入custom_settings设置。 例如： spiders/somespider.py from ..custom_settings import * class Spider1(CrawlSpider): name = \"spider1\" custom_settings = custom_settings_for_spider1 pass custom_settings.py custom_settings_for_spider1 = { 'LOG_FILE': 'spider1.log', 'CONCURRENT_REQUESTS': 100, 'DOWNLOADER_MIDDLEWARES': { 'spider.middleware_for_spider1.Middleware': 667, }, 'ITEM_PIPELINES': { 'spider.mysql_pipeline_for_spider1.Pipeline': 400, }, } 在spider里有两个蜘蛛spider1、spider2里，我们引入了来自custom_settings的配置变量custom_settings_for_spider1、custom_settings_for_spider2，通过这些变量，我们分别对两个爬虫的log文件、并发数、应用的中间件和管道文件进行了设置。 custom_settings的优先级在命令行以下，比settings.py要高。 settings的使用技巧 在这简单说说我在工作中对于不同类型settings的使用技巧吧。 1.首先是settings.py文件，在一个scrapy项目中，一些通用的设置，比如请求头、代理入口、数据库连接等等，都可以统一写在settings.py中。 2.其次是custom_settings，在scrapy项目中单独建立一个custom_settings.py文件，依据不同spider所需要的设置，比如某站点A可能反爬严，我并发设置短点，站点B没反爬，我并发设置高点；又或者A用了中间件MA，B用了中间件MB等，我在custom_settings.py文件中分别给予设置。 3.最后是命令行。例如我要每个进程的日志分别查看，或者追踪每个进程的爬取速度等，这用命令行参数就比较合适。例如： from scrapy import cmdline cmdline.execute('scrapy crawl spider1 -s LOG_FILE=p1.log -s PROCESS_NAME=1'.split()) 分布式爬取 第三方库 scrapy-redis 为scrapy框架扩展了分布式爬取的功能，使用redis + scrapy-redis 实现。 分布式爬虫框架需解决： 分配爬取任务：为每个爬虫分配不重复的爬取任务。 汇总爬取数据：将所有爬虫爬取到的数据汇总到一处。 scrapy-redis 让所有爬虫共享一个存在于redis中的请求队列（代替原先各个爬虫的独立请求队列）。 scrapy-redis 实现了以下组件： 基于redis的请求队列：优先队列（默认）、FIFO、LIFO 基于redis的请求去重过滤器：过滤掉重复请求。 基于以上两个组件的调度器。 分布式爬取使用： 修改原scrapy项目的爬虫文件： 使爬虫继承 \"scrapy_redis.spiders.RedisSpider\"，而不是原来的 \"scrapy.Spider\" 注释掉 start_urls。 修改配置文件： # settings.py REDIS_URL = 'redis://1.2.3.4:6379' # 可远程访问 SCHEDULER = 'scrapy_redis.scheduler.Scheduler' DUPEFILTER_CALSS = 'scrapy_redis.dupefilter.RFPDupeFilter' # 去重 ITEM_PIPELINES = { 'scrapy_redis.pipelines.RedisPipeline':300 } # 爬虫停止后，是否清除redis中的请求队列、去重集合。默认为False，即清除。 SCHEDULER_PERSIST = False \"\"\" REDIS_START_URLS_KEYS = ':start_urls' # redis的键，从该队列中获取初始爬取链接 存储爬取的item键：':items'，是一个list 存储去重集合的键： ':dupefilter'，是一个set \"\"\" 将源文件部署到若干台机器上，并启动爬虫。此时请求队列是空的，爬虫都处于等待状态。 打开redis，放入初始链接： lpush :start_urls 'https://www.baidu.com' 爬虫开始爬取数据。 scrapy使用注释进行调试 contract有点像为爬虫设计的单元测试，可以让你快速知道哪里有运行异常。 主要实现在 scrapy.contracts.deafult.py中。 def parse(self, response): \"\"\" This function parses a property page. @url http://xx.com/xx.html @returns items 1 @scrapes title price description address image_urls @scrapes url project spider server date \"\"\" pass 上述代码的含义是：检查该url，并找到列出字段中有值的一个item。 测试某个爬虫（根据上面的注释）： scrapy check 爬虫部署 Scrapyd是官方开发的，用于部署、运行和管理scrapy的工具。scrapyd没有权限管理（即不能设置密码）。 首先先将scrapyd安装在云服务器上： pip install scrapyd pip install scrapyd-client 配置文件： # /etc/scrapyd/scrapyd.conf [scrapyd] bind_address = [云服务器的外网ip] 启动，直接键入以下命令： scrapyd 就可以进行访问了：http://[云服务器的外网ip]:6800。 部署爬虫： 打开项目的scrapy.cfg文件，将第10行的url改为上面的url。 然后键入以下命令进行部署： scrapyd-deploy # 在项目根目录下打开命令行 启动爬虫： curl http://[host]:6800/schedule.json -d project=[project_name] -d spider=[spider_name] 启动后，就可以在网页上查看相关情况了。 结束爬虫： curl http://[host]:6800/cancel.json -d project=[project_name] -d spider=[spider_job_id] telnet telnet 监测爬虫运行，并且可以运行python代码 连接telnet：telnet localhost 6023(or 6024) 常用命令： est ( ) : 查看爬虫引擎各组件的运行状态 p(stats.get_stats()) ：查看爬虫已经运行的各项指标 :coffee: :zapple Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "},"爬虫/常见问题.html":{"url":"爬虫/常见问题.html","title":"常见问题","keywords":"","body":"常见问题 [TOC] win10平台上安装与运行scrapy出错 No module named 'win32api' win10平台上安装与运行scrapy出错 直接执行pip install scrapy安装会出现如下错误： 错误节选： ERROR: Failed building wheel for Twisted error: [WinError 3] 系统找不到指定的路径。: 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\PlatformSDK\\\\lib' 这说明是Twisted安装失败，失败原因是缺少Microsoft Visual Studio 14.0，因此可以先安装该软件，再安装scrapy。但该方法会比较差。。。。 另外一种方法是下载已经编译好的Twisted包（whl文件），然后使用pip安装。（先使用pip install scrapy是先下载源代码，然后再本地进行编译，由于缺少相关的库，编译失败。） 可以在该网址下载：https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 根据python版本和自己电脑的型号选择合适的版本，我选的是python3.7的64位版本。 然后使用pip安装该包： pip install Twisted-19.2.1-cp37-cp37m-win_amd64.whl 接下来就能正常安装scrapy了 No module named 'win32api' 在新建项目以后，运行爬虫出现如下错误： scrapy crawl demo # 错误： # ModuleNotFoundError: No module named 'win32api' 解决方法： pip install pypiwin32 Copyright @appwhy all right reserved，powered by Gitbook文件更新于： 2020-06-17 09:37:25 "}}